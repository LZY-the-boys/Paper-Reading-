# Paper-Reading

## Image2Video

- SOTA
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/dc652825-7ee1-449a-9d4d-4cc8abbf4cf1/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/bf75ea13-0154-4028-87cc-5f85b3f666b6/image.png)
    
    - [T2V-Turbo-v2](https://arxiv.org/pdf/2410.05677)
        - VidGen-1M (VG), OpenVid-1M (OV), WebVid-10M (WV),
            - find that VCM perform best on OV, their method best on VG+WV
        - motion guidance (MG)
            - we only apply motion guidance to the first Ï„ percent of the sampling t (Motion Clone)
    - [Stable-Video-Diffusion](https://arxiv.org/pdf/2311.15127)
        - ä¸‰é˜¶æ®µè®­ç»ƒ
            - 2D text-video-pretraining,(ç›´æ¥ä½¿ç”¨sd2.1)
            - video pretraining on a large dataset at low resolution,(è‡ªå·±æ ‡äº†LVDï¼Œå¹¶è¿‡æ»¤)
            - high-resolution video finetuning on a much smaller dataset with higher-quality videos (250k)
        - æ£€æµ‹è§†é¢‘æ˜¯å¦åŠ¨æ€ï¼š optical flow
    - [DynamiCrafter](https://arxiv.org/pdf/2310.12190)
        - 3 stage paradigmï¼š
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/a17b5f66-a8eb-47e9-b9da-9a343020a5fd/image.png)
            
            - trained on WebVid10M dataset by sampling 16 frames with dynamic FPS
            at the resolution of 256 Ã— 256 in a batch
            - (i) training P  ï¼ˆ=query transformerï¼‰ç”¨çš„æ˜¯SDï¼Œåªåštext2image
                - æ¨¡å‹: P + cross-attn +  Stable-Diffusion-v2.1ï¼ˆSD)
                å­¦ä¹ ç‡ï¼š1 Ã— 10^-4
                æ‰¹æ¬¡å¤§å°ï¼š64
                è®­ç»ƒæ­¥æ•°ï¼š1000K step ï¼ˆçº¦64M imgï¼‰
            - (ii) adapting P to the T2V model æŠŠSD æ¢æˆ text2videoæ¨¡å‹
                - æ¨¡å‹: P + T2Vï¼ˆreplace SD with VideoCrafterï¼‰
                å­¦ä¹ ç‡ï¼š5 Ã— 10^-5
                æ‰¹æ¬¡å¤§å°ï¼š64
                è®­ç»ƒæ­¥æ•°ï¼š30K step ï¼ˆçº¦ 1M imgï¼‰
            - (iii) joint fine-tuning with VDG. å†åŠ å…¥imgæ¡ä»¶ï¼Œè½¬æˆimg2videoæ¨¡å‹
                - æ¨¡å‹: åœ¨å‰ä¸¤ä¸ªé˜¶æ®µçš„åŸºç¡€ä¸Šï¼ŒåŠ å…¥image-noise-concat (VDGï¼‰è¿›è¡Œè”åˆå¾®è°ƒã€‚
                å­¦ä¹ ç‡ï¼š5 Ã— 10^-5
                æ‰¹æ¬¡å¤§å°ï¼š64
                è®­ç»ƒæ­¥æ•°ï¼š100Kæ­¥ ï¼ˆçº¦ 6M imgï¼‰
            - Ablationï¼šå•çœ‹FVD / PICæŒ‡æ ‡ï¼Œæ²¡æœ‰iiiæ˜¯æœ€å¥½çš„ï¼Œä½†æ˜¯å®é™…ç”Ÿæˆçš„æ—¶å€™ä¼šå­˜åœ¨å˜åŒ–å¹…åº¦å°ï¼Œå˜å½¢ä¸¥é‡ç­‰é—®é¢˜ï¼› æ²¡æœ‰iä¼šå¯¼è‡´æ”¶æ•›éå¸¸æ…¢ï¼Œæ²¡æœ‰iiå®¹æ˜“è®­ç»ƒå´©æºƒ
            - å…³äº
            
            fps = frame per second = num of image per second
            8fps 2sçš„å›¾ç‰‡ï¼Œvideo length=8*2=16ï¼Œ è¡¨ç¤ºä»è§†é¢‘ä¸­æå–çš„å¸§æ•°
            frame_strideæ˜¯æå–å¸§ä¹‹é—´çš„æ­¥é•¿ï¼ŒåŸå§‹è§†é¢‘æ˜¯24fpsï¼Œfs=3æ—¶å°±æ˜¯24/3=8fps
            
            video lengthæœ‰æ›´é«˜ä¼˜å…ˆçº§
            frame_stride ä»…è®¾ç½®æœ€å¤§å€¼ï¼Œä¼šè‡ªåŠ¨è®¡ç®—å®é™…åº”è¯¥ç”¨çš„å€¼
            
            ```python
            frame_stride = random.randint(self.frame_stride_min, self.frame_stride)
            required_frame_num = frame_stride * (self.video_length-1) + 1
            frame_stride=3,video length=16,å®é™…éœ€è¦è§†é¢‘è‡³å°‘æœ‰46å¸§
            frame_num = len(video_reader)
            if frame_num < required_frame_num:
                ## drop extra samples if fixed fps is required
                if self.fixed_fps is not None and frame_num < required_frame_num * 0.5:
                    index += 1
                    continue
                else:
                    frame_stride = frame_num // self.video_length
                    required_frame_num = frame_stride * (self.video_length-1) + 1
            ```
            
            - Motion Ctrlï¼š ä¸“é—¨æ ‡æ³¨åŠ¨æ€/è¿åŠ¨ä¿¡æ¯ https://huggingface.co/datasets/Doubiiu/webvid10m_motion/blob/main/webvid10m_motion.csv ï¼ˆ2.57Mï¼‰
                - è¿‡æ»¤æ•°æ®ï¼šä»æ•°æ®é›†ä¸­è¿‡æ»¤æ‰åŒ…å«è¾ƒå¤§ç›¸æœºç§»åŠ¨ã€è¾ƒå·®çš„å­—å¹•-è§†é¢‘å¯¹é½ï¼ˆcaption-video alignmentï¼‰ä»¥åŠå›¾å½¢/CGIå†…å®¹çš„æ•°æ®ã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ç¡®ä¿æ•°æ®é›†ä¸­åªåŒ…å«é‚£äº›å…·æœ‰çœŸå®åŠ¨æ€åœºæ™¯çš„è§†é¢‘ï¼Œé¿å…å› ç‰¹æ•ˆæˆ–ç›¸æœºç§»åŠ¨ç­‰å› ç´ å¯¼è‡´çš„è¯¯åˆ¤ã€‚
                - ç”ŸæˆåŠ¨æ€ç½®ä¿¡åº¦å’ŒåŠ¨æ€æè¿°ï¼šä½¿ç”¨GPT4æ¨¡å‹æ¥ç”ŸæˆåŠ¨æ€ç½®ä¿¡åº¦ï¼ˆdynamic confidenceï¼‰ã€åŠ¨æ€æè¿°ï¼ˆdynamic wordingï¼‰ä»¥åŠåŠ¨æ€åœºæ™¯çš„ç±»åˆ«ï¼ˆcategoryï¼‰ã€‚
                åŠ¨æ€ç½®ä¿¡åº¦è¡¨ç¤ºå­—å¹•æè¿°åŠ¨æ€åœºæ™¯çš„ç¨‹åº¦ï¼ŒåŠ¨æ€æè¿°åˆ™æ˜¯å…·ä½“çš„åŠ¨ä½œæè¿°ï¼Œå¦‚â€œäººåšä¿¯å§æ’‘â€ï¼Œè€Œç±»åˆ«åˆ™æ˜¯å¯¹è¯¥åŠ¨æ€åœºæ™¯çš„åˆ†ç±»ï¼Œå¦‚äººç±»ï¼ˆhumanï¼‰ã€‚
                æŒ‡ä»¤è¯´æ˜ï¼š
                ç”¨æˆ·æŒ‡ä»¤è¦æ±‚GPT4æ¨¡å‹æ£€æŸ¥å­—å¹•æ˜¯å¦æè¿°äº†è§†é¢‘ä¸­çš„åŠ¨æ€åœºæ™¯ï¼Œä¾‹å¦‚äººç±»æˆ–åŠ¨ç‰©çš„åŠ¨ä½œç­‰ã€‚
                è¾“å‡ºåŠ¨æ€ç½®ä¿¡åº¦ï¼Œä»0åˆ°100è¡¨ç¤ºç½®ä¿¡åº¦ç­‰çº§ï¼Œ0è¡¨ç¤ºæœ€ä½ç½®ä¿¡åº¦ï¼Œ100è¡¨ç¤ºæœ€é«˜ç½®ä¿¡åº¦ã€‚
                è¾“å‡ºåŠ¨æ€æè¿°ï¼Œå¦‚â€œäººåšä¿¯å§æ’‘â€ï¼Œä»¥åŠè¯¥åŠ¨æ€åœºæ™¯çš„ç±»åˆ«ã€‚
                - æ„å»ºçš„æ•°æ®é›†åŒ…å«å¤§çº¦**257ä¸‡æ¡å­—å¹•-è§†é¢‘å¯¹ã€‚**
                å‘ç°åŠ¨æ€ç½®ä¿¡åº¦ä¸º40æ—¶ï¼Œä¸äººç±»åˆ¤æ–­æœ€ä¸ºä¸€è‡´ã€‚
    - [Vchitect2.0](https://vchitect.intern-ai.org.cn/#section1) Coming Soon
    - Venhancer
        - è‡ªå·±æ”¶é›†çš„ We collect around 350k high-quality and high-resolution video clips from the Internet to constitute our training set
    - Vchitect1.0 [LaVie](https://arxiv.org/pdf/2309.15103)
        - Vimeo25M æœªå…¬å¼€
    - [meta] [Mar-Video](https://arxiv.org/pdf/2410.20280)
        - ç›¸æ¯”marè€Œè¨€ï¼Œæ˜¯æŠŠpatchæ¢æˆäº†frame
        - å’Œæ™®é€šçš„I2Væ¨¡å‹ä¸åŒï¼ŒMARDiniä¸ç›´æ¥ä½¿ç”¨imgä½œä¸ºconditionï¼Œè€Œæ˜¯å¼•å…¥low resolutionå’Œhigh resolution videoçš„åŒè·¯è¾“å…¥ï¼Œå¯¹low resolutionæå–ç‰¹å¾ä½œä¸ºconditionï¼Œåœ¨high resolutionä¸Šdiffusion
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/2bae166e-d37f-4f7a-8d35-e308916ef49c/image.png)
        
        - (é‡å‹1.3B) MARè¿›è¡Œæ—¶é—´è§„åˆ’ï¼ˆtemporal planningï¼‰å’Œï¼ˆè½»é‡çº§ 288Mï¼‰DiffusionModelè¿›è¡Œç©ºé—´ç”Ÿæˆ ï¼ˆspatial ï¼‰
            - MARè§„åˆ’æ¨¡å‹**æ©ç æ ‡è®°**ä½åˆ†è¾¨ç‡çš„è¾“å…¥å¸§ï¼Œéšæœºé‡‡æ ·K'å¸§æ›¿æ¢æˆ[MASK], ç”Ÿæˆè§„åˆ’ä¿¡å·z_cond
            - è€ŒDMç”Ÿæˆæ¨¡å‹ä½¿ç”¨z_condæ¥é€šè¿‡**æ©ç æ ‡è®°**æ‰©æ•£å»å™ªäº§ç”Ÿé«˜åˆ†è¾¨ç‡å¸§ï¼Œ
            å¯¹K'å¸§è¿›è¡Œé‡‡æ ·å¹¶æ·»åŠ å™ªå£°ï¼Œç”Ÿæˆè¢«æ©ç çš„å¸§[NOISE]ï¼Œè€Œä¿ç•™å…¶ä½™çš„K - K'å‚è€ƒå¸§[REF], G(Z_noise,t, Z_cond)
            - å˜é™æ€é—®é¢˜ï¼š ä¸ºäº†å¤„ç†[REF]å’Œ[NOISE]é›†æˆåˆ°ä¸€ä¸ªåºåˆ—ä¸­å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šæ€§ï¼Œå¼•å…¥äº†Identity Attentionå¦‚å³
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/6e478e82-422c-4750-ae8f-99ffee8386bd/image.png)
        
        - è®­ç»ƒä¸‰é˜¶æ®µ
            - åˆ†åˆ«è®­ç»ƒMARå’ŒDMï¼Œä½¿ç”¨æ©ç æ‰©æ•£æŸå¤±
            - åœ¨ç®€å•çš„**è§†é¢‘æ’å€¼**ä»»åŠ¡ä¸Šè”åˆè®­ç»ƒæ¨¡å‹ï¼Œåªä½¿ç”¨æ©ç æ‰©æ•£æŸå¤±
            - é€šè¿‡é€æ¸å‡å°‘ä¿ç•™çš„å‚è€ƒå¸§æ•°é‡ï¼Œè¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè”åˆå­¦ä¹ è§†é¢‘æ’å€¼å’Œ**å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ**ä»»åŠ¡
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/cbb51232-cf91-4491-a104-6060abbbd790/image.png)
        
    - [NIPS24][I2Væ¨¡å‹SFTé™æ­¢é—®é¢˜] [Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model](https://arxiv.org/pdf/2406.15735)   https://github.com/thu-ml/cond-image-leakage
        - è®¤ä¸ºSFTåI2Væ¨¡å‹è¶‹å‘äºé™æ­¢æ˜¯å› ä¸ºæ¡ä»¶å›¾åƒæ³„éœ²ï¼ˆConditional Image Leakage, CILï¼‰åœ¨è¾ƒå¤§çš„æ—¶é—´æ­¥éª¤ä¸Šè¿‡åº¦ä¾èµ–æ¡ä»¶å›¾åƒï¼Œè€Œå¿½ç•¥äº†ä»å¸¦å™ªå£°çš„è¾“å…¥ä¸­é¢„æµ‹å¹²å‡€è§†é¢‘çš„å…³é”®ä»»åŠ¡ã€‚è¿™å¯¼è‡´ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹åŠ¨æ€å’Œç”ŸåŠ¨çš„è¿åŠ¨
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/e2069e52-6d7b-4ebf-957c-fc10c4444921/image.png)
            
        - **inference strategy:**
            - ç›´è§‚çš„ï¼Œåœ¨åŠ å™ªå¤šçš„åœ°æ–¹ï¼Œæ¨¡å‹ä¼šæ›´ä¾èµ–conditional img. æ‰€ä»¥æˆ‘ä»¬é¦–å…ˆå¯ä»¥ä»æ¯”è¾ƒå°çš„æ—¶é—´æ­¥ï¼ˆåŠ å™ªå°‘ï¼‰å¼€å§‹é‡‡æ ·ï¼Œæœ¬æ¥T=1000ï¼Œç°åœ¨ç”¨800
            - **Analytic Noise Initialization (Analytic-Init)** ç”±äºç°åœ¨Tå˜å°äº†ï¼Œéœ€è¦è°ƒæ•´ä¸€ä¸‹é‡‡æ ·çš„muå’Œsigmaé¿å…é™ä½å›¾åƒè´¨é‡ï¼Œç”¨æœ€å°åŒ–KLæ•£åº¦åŸå§‹Tåœ¨æŸtçš„åˆ†å¸ƒå’Œç°åœ¨tçš„åˆ†å¸ƒå¯ä»¥æ¨å¯¼
        - **training strategy:**
            - ç›´è§‚çš„ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹conditional image(y0)åŠ å™ª, ä½†æ˜¯å­˜åœ¨ä¸€ä¸ªcontent consistencyå’Œleakageçš„tradeoffï¼Œè¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œåœ¨å¤§æ—¶é—´æ­¥æˆ‘ä»¬åº”è¯¥å¯¹y0å¤šåŠ noiseï¼Œåœ¨å°æ—¶é—´æ­¥åº”è¯¥å°‘åŠ ä¿è¯contentçš„consistencyï¼Œæäº†ä¸€ç§TimeNoiseç­–ç•¥ï¼Œinferenceçš„æ—¶å€™è¿˜æ˜¯å›ºå®šçš„noise
- Open-Domain Video-Generation Dataset
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/e3dafd0c-b48c-4ee7-a2ee-f1aff60d59d8/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/7b2b9436-bbe9-4675-bacb-b71edf93d632/image.png)
    
    - [ICCV 2021] [webvid10M](https://arxiv.org/pdf/2104.00650)
    - [**InternVid**](https://arxiv.org/abs/2307.06942)
    - [CVPR 24] [**Panda-70M**](https://arxiv.org/abs/2406.18522) a curated subset of HDVILA-100M
    - [ICLRåœ¨æŠ•] [VIDGEN-1M](https://arxiv.org/pdf/2408.02629)
    - [NIPSæ²¡ä¸­ICLRåœ¨æŠ•] [OpenVid](https://export.arxiv.org/pdf/2407.02371)
    - [NIPS 2024] [LVD-2M](https://arxiv.org/abs/2410.10816)
    - [NIPS 2024] [VidProM](https://arxiv.org/pdf/2403.06098) prompt-datasetï¼Œ videoæ˜¯æ¨¡å‹ç”Ÿæˆçš„ï¼Œç›¸å½“äº[ACL23 best paper] DiffusionDBçš„videoç‰ˆæœ¬
    - [Report 2024] â€£ [2407.11784](https://arxiv.org/pdf/2407.11784)
- benchmark
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/1efca875-8783-482b-a43f-5ce0b3331368/image.png)
    
    - [NIPS 24 spotlight] [ChronoMagic](https://arxiv.org/pdf/2406.18522) benchmark åº”è¯¥å¯ä»¥å½“æˆè®­ç»ƒæ•°æ®é›†æ¥ç”¨
    - vbench
        - â€£
        - imaging_qualityç”¨musiq_spaq_ckp
        - aesthetic_qualityç”¨ViT-L-14
        - background_consistencyç”¨ViT-B-32
        - camera_motionç”¨cotracker2
        - i2v_subjectç”¨dino_vitb16
- Motion-Control / Guidance
    - Boosting Camera Motion Control for Video Diffusion Transformers
    - Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models
    - CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility
    - CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities
    - FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance
    - JVID: Joint Video-Image Diffusion for Visual-Quality and Temporal-Consistency in Video Generation
    - MotionCtrl: A Unified and Flexible Motion Controller for Video Generation
    - MotionClone
    - VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models
    - [ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution](https://arxiv.org/abs/2410.13807)
    - StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text
- Survey
    - Video Diffusion Models: A Survey
- Decouple / Attention
    
    FreeLong Training-Free Long Video Generation with SpectralBlend Temporal Attention
    
    Differential Transformer
    
- FrameDependent
    - the content redundancy and temporal correlations among different frames, so we should apply different noise prior to different frame ?
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/a6f926dd-a534-419b-8934-14477c87f75a/image.png)
    
    - æ€è€ƒï¼šå…¶å®å¾ˆåƒllmé‡Œè¾¹çš„ä½ç½®è¡°å‡ï¼ŒåŠ å¼ºï¼ˆå’Œç¬¬ä¸€å¸§çš„ï¼‰ä½ç½®ç¼–ç ä¿¡æ¯ç†è®ºä¸Šæœ‰ç”¨ï¼Œç„¶åæ­£å¦‚é•¿æ–‡æœ¬åœºæ™¯ä¸€æ ·ï¼Œå¯¹long videoè‡³å…³é‡è¦
    - [CVPR24] [PIA](https://arxiv.org/pdf/2312.13964)
        - the L1 distance between each frame and 1st frame, concat this info with conditional frame
    - [CVPR23] [VideoFusion](https://arxiv.org/abs/2303.08320)
        - decomposes the diffusion process using **a shared base noise for each frame and residual noise along the temporal axis**. This noise decomposition is achieved through two co-training networks
    - [ICLR24] [CMD](https://arxiv.org/pdf/2403.14148)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/b9cb9a80-b010-4e18-b39e-3490e96bcaaf/image.png)
        
        - decompose the diffusion process to motion diffusion and content diffusion
    - [ICCV23] [PYoCo](https://arxiv.org/pdf/2305.10474)
        - modify the noise process to preserve the **correlation between different frames**
        - mixed: noise = Ïµshared + Ïµind
        - progressive: noise at frame i is generated by perturbing the noise at frame i âˆ’ 1
    - [ICLR24] [SEINE](https://arxiv.org/pdf/2310.20700)
        - å»ºæ¨¡æˆä¸€ä¸ªMask Modelingé—®é¢˜ï¼Œç”¨ç±»ä¼¼mlmçš„æ–¹å¼å­¦ä¹ æ’å¸§
    - [ICLR24] [FreeNoise](https://arxiv.org/pdf/2310.15169)
        - **é•¿è§†é¢‘ç”Ÿæˆçš„æŒ‘æˆ˜**ï¼šç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹é€šå¸¸åœ¨æœ‰é™çš„å¸§æ•°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¼è‡´åœ¨æ¨ç†é˜¶æ®µæ— æ³•ç”Ÿæˆé«˜ä¿çœŸçš„é•¿è§†é¢‘ã€‚è¿™æ˜¯å› ä¸ºé•¿è§†é¢‘åœ¨è®­ç»ƒé˜¶æ®µæ²¡æœ‰å¾—åˆ°ç›‘ç£ã€‚
        - **å•æ–‡æœ¬æ¡ä»¶é™åˆ¶**ï¼šç°æœ‰çš„æ¨¡å‹ä»…æ”¯æŒå•æ–‡æœ¬æ¡ä»¶ï¼Œè€Œç°å®ç”Ÿæ´»ä¸­çš„åœºæ™¯é€šå¸¸éœ€è¦å¤šæ–‡æœ¬æ¡ä»¶ï¼Œå› ä¸ºè§†é¢‘å†…å®¹ä¼šéšç€æ—¶é—´å˜åŒ–ã€‚
        - **[æš‚æ—¶æ²¡çœ‹æ‡‚]** é‡æ–°å®‰æ’ä¸€ç³»åˆ—å™ªå£°ä»¥å®ç°é•¿è·ç¦»ç›¸å…³æ€§ï¼Œå¹¶é€šè¿‡åŸºäºçª—å£çš„èåˆå¯¹å®ƒä»¬è¿›è¡Œæ—¶é—´æ³¨æ„åŠ›å¤„ç†,  å¯¹å›ºå®šéšæœºå™ªå£°å¸§åºåˆ—è¿›è¡Œå±€éƒ¨æ´—ç‰Œï¼Œç”Ÿæˆå…·æœ‰å†…éƒ¨éšæœºæ€§å’Œé•¿è·ç¦»ç›¸å…³çš„å™ªå£°å¸§åºåˆ—ï¼Œåœ¨ä¸å¯¹è±¡å½¢çŠ¶ç›¸å…³çš„æ—¶é—´æ­¥éª¤ä¸­é€æ¸æ³¨å…¥æ–°çš„åŠ¨ä½œ
    - [ICLR24 oral (8 8 8 6)] [$\int$-noise](https://openreview.net/forum?id=pzElnMrgSD) âˆ«-noise
        - ä¸€ç§ç±»ä¼¼äºVARçš„å±‚çº§noise
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/b08c471e-5504-47e2-9814-371178c517b5/image.png)
        
        - èƒŒæ™¯1ï¼›å‰é¢é‚£æ®µBorelé›†å¯ä»¥ä¸çœ‹ï¼ŒåŸºæœ¬æ„æ€æ˜¯DÃ—Då¤§å°çš„ç¦»æ•£å™ªå£°å›¾å¯ä»¥è§†ä¸º[0,D]Ã—[0,D]åŒºåŸŸä¸Šçš„è¿ç»­å™ªå£°å›¾çš„ç§¯åˆ†ï¼ŒåŒæ—¶å¯ä»¥é€šè¿‡é€æ­¥ç»†åˆ†å­åƒç´ çš„æ–¹å¼æŠŠDÃ—Dç¦»æ•£æŒ‰2áµDÃ—2áµDé€æ¸æ‰©å±•åˆ°è¿ç»­æƒ…å†µ
        - èƒŒæ™¯2ï¼›å…‰æµå›¾ï¼ˆOptical Flowï¼‰æ˜¯æŒ‡åœ¨è¿ç»­çš„ä¸¤å¸§å›¾åƒä¸­ï¼ŒåŒä¸€ç‰©ä½“çš„åƒç´ ç‚¹åœ¨æ—¶é—´ä¸Šçš„ä½ç§»ã€‚ä¸€ä¸ªå…‰æµå›¾å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º `(H, W, 2)` çš„å¼ é‡ï¼Œå…¶ä¸­ `H` å’Œ `W` åˆ†åˆ«æ˜¯å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ï¼Œç¬¬ä¸‰ä¸ªç»´åº¦çš„ä¸¤ä¸ªé€šé“åˆ†åˆ«è¡¨ç¤ºæ°´å¹³å’Œå‚ç›´æ–¹å‘çš„ä½ç§»ã€‚
            - å¯ä»¥ç†è§£ä¸ºç‰©ä½“ç§»åŠ¨è§£ç®—æ–¹æ³•ï¼Œæœ¬æ–‡ç”¨äºæ±‚è§£æ¡ä»¶è§†é¢‘æ‰€å¯¹åº”çš„æ—¶åºæ˜ å°„ï¼ˆå˜å½¢å‘é‡åœºï¼‰${\cal T}$
        - æ—¶åºä¸€è‡´æ€§æ˜¯æŒ‡ä¸åŒå¸§ä¹‹é—´çš„å†…å®¹å’Œç»“æ„ä¿æŒä¸€è‡´ï¼Œä¸»è¦æ˜¯æœ‰ä»¥ä¸‹å‡ ä¸ªæŒ‘æˆ˜
            - LDMsä¸­çš„å™ªå£°æ ·æœ¬åˆ†è¾¨ç‡è¾ƒä½ï¼Œä¸»è¦æ§åˆ¶å›¾åƒçš„ç»„æˆå’Œä½é¢‘ç»“æ„ï¼Œè€Œä¸æ˜¯é«˜é¢‘ç»†èŠ‚ã€‚è¿™é™åˆ¶äº†å™ªå£°æ‰­æ›²åœ¨ä¼ é€’è¿åŠ¨ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚
            - æ—¶åºä¸€è‡´çš„å›¾åƒä¸ä¸€å®šè½¬åŒ–ä¸ºæ—¶åºä¸€è‡´çš„è‡ªç¼–ç å™¨æ½œåœ¨å‘é‡ã€‚å› æ­¤ï¼Œæ—¶åºä¸€è‡´çš„å™ªå£°å…ˆéªŒåœ¨æ½œåœ¨ç©ºé—´ä¸­å¯èƒ½æ˜¯æ¬¡ä¼˜çš„
            - ä¼ ç»Ÿçš„å™ªå£°é‡‡æ ·æŠ€æœ¯é€šå¸¸ä¸¤ç§
                - ç‹¬ç«‹åœ°ä¸ºæ¯ä¸€å¸§ç”Ÿæˆå™ªå£°æ ·æœ¬ã€‚ç”±äºæ¯ä¸€å¸§çš„å™ªå£°æ ·æœ¬æ˜¯ç‹¬ç«‹çš„ï¼Œ**ç›¸é‚»å¸§ä¹‹é—´çš„å™ªå£°ç¼ºä¹ç›¸å…³æ€§**ï¼Œå¯¼è‡´è§†é¢‘ä¸­å¯èƒ½å‡ºç°å¿«é€Ÿé¢‘ç¹çš„ä¸è¿ç»­çš„å˜åŒ–ï¼ˆé«˜é¢‘é—ªçƒï¼ˆhigh-frequency flickeringï¼‰ï¼Œå¦‚æŸç‰¹å¾çªç„¶å‡ºç°/æ¶ˆå¤±ï¼‰ï¼›
                - **å›ºå®šå™ªå£°ä¿¡å·åœ¨æ‰€æœ‰å¸§ä¸­ä¿æŒä¸å˜ï¼ˆfixed noiseï¼‰**æ¥å¼ºåˆ¶ä¿è¯ç›¸å…³æ€§ï¼Œä¼šå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘ä¸­æŸäº›çº¹ç†æˆ–ç»†èŠ‚åœ¨ä¸åŒå¸§ä¹‹é—´å›ºå®šä¸åŠ¨ã€‚ï¼ˆçº¹ç†ç²˜è¿ä¼ªå½±ï¼ˆtexture-sticking artifactsï¼‰ï¼Œå¦‚æŸç§çº¹ç†å®šæ ¼åœ¨åƒç©ºé—´ä¸­ï¼‰
                - è¿™äº›é—®é¢˜éš¾ä»¥é€šè¿‡ä¼ ç»Ÿçš„å¹³æ»‘æ»¤æ³¢æŠ€æœ¯ï¼ˆå¦‚æ—¶é—´åŸŸæˆ–ç©ºé—´åŸŸçš„å¹³æ»‘æ»¤æ³¢ï¼‰ç­‰åå¤„ç†æŠ€æœ¯è¯†åˆ«å’Œä¿®æ­£
        - **âˆ«-å™ªå£°è¡¨ç¤º**ï¼šæœ¬æ–‡æå‡ºå°†ç¦»æ•£çš„å™ªå£°æ ·æœ¬é‡æ–°è§£é‡Šä¸ºè¿ç»­å™ªå£°åœºçš„ç§¯åˆ†ï¼Œå³æ¯ä¸ªåƒç´ çš„å€¼ä¸å†æ˜¯ç¦»æ•£çš„å™ªå£°å€¼ï¼Œè€Œæ˜¯è¯¥åƒç´ åŒºåŸŸå†…çš„è¿ç»­å™ªå£°åœºçš„ç§¯åˆ†å€¼ã€‚è¿™ç§é‡æ–°è§£é‡Šå…è®¸åœ¨é«˜åˆ†è¾¨ç‡ä¸‹ç”Ÿæˆå™ªå£°æ ·æœ¬æ—¶ä¿æŒå™ªå£°åœºçš„ç‰¹æ€§ï¼Œä»è€Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ›´å¥½åœ°ä¿æŒæ—¶åºä¸€è‡´æ€§ã€‚
        - **å™ªå£°ä¼ è¾“æ–¹ç¨‹**ï¼šé€šè¿‡å…‰å­¦æµï¼ˆoptical flowï¼‰æˆ–å˜å½¢åœºï¼ˆdeformation fieldï¼‰å°†å‰ä¸€å¸§çš„å™ªå£°æ ·æœ¬ä¼ é€’åˆ°å½“å‰å¸§ï¼ŒåŒæ—¶ä¿æŒå™ªå£°çš„ç»Ÿè®¡ç‰¹æ€§ã€‚è¿™ç¡®ä¿äº†ä¸åŒå¸§ä¹‹é—´çš„å™ªå£°ç›¸å…³æ€§ï¼Œä»è€Œåœ¨æ½œåœ¨ç©ºé—´ä¸­ä¿æŒæ—¶åºä¸€è‡´æ€§
        - é€šè¿‡ä»ä½åˆ†è¾¨ç‡å™ªå£°æ ·æœ¬ç”Ÿæˆé«˜åˆ†è¾¨ç‡å™ªå£°æ ·æœ¬ï¼Œå¯ä»¥åœ¨ä¿æŒæ—¶åºç›¸å…³æ€§çš„åŒæ—¶ï¼Œå¼•å…¥æ›´å¤šçš„ç»†èŠ‚ä¿¡æ¯
        - å…·ä½“æ­¥éª¤
            - ç”Ÿæˆä¸€ä¸ªé«˜åˆ†è¾¨ç‡çš„é«˜æ–¯å™ªå£°å›¾ å°†é«˜æ–¯å™ªå£°å›¾é‡æ–°è§£é‡Šä¸ºä¸€ä¸ªç§¯åˆ†å™ªå£°åœº
            - ä½¿ç”¨ç°æœ‰çš„optical transportæ–¹æ³• PWC-Net [3] or RAFT [4] è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„å…‰æµå›¾ã€‚ä¹Ÿå°±æ˜¯é€†å˜å½¢åœº $T^{-1}$
            - **ä¸Šé‡‡æ ·å…‰æµå›¾**ï¼šä½¿ç”¨åŒä¸‰æ¬¡æ’å€¼ï¼ˆbicubic interpolationï¼‰å°†ä½åˆ†è¾¨ç‡çš„å…‰æµå›¾ä¸Šé‡‡æ ·åˆ°é«˜åˆ†è¾¨ç‡  $W(T^{-1}(x))$
            - **æ‰­æ›²å™ªå£°**ï¼šä½¿ç”¨ä¸Šé‡‡æ ·åçš„å…‰æµå›¾å°†å™ªå£°ä»ä¸€å¸§æ‰­æ›²åˆ°å¦ä¸€å¸§     
$$
T_A(W) = \int_{x \in A} \frac{1}{|\nabla T(T^{-1}(x))|^{\frac{1}{2}}} W(T^{-1}(x)) \, dx
$$
        - ç¼ºç‚¹ï¼š
            - è®¡ç®—å¼€é”€
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81e645bc-1e8a-41e9-a2c2-29d1d1d98df5/71cd23e7-b84f-43cf-8273-043a732c1ebe/image.png)
            
            - éšå¼å‡è®¾æ—¶é—´ç›¸å…³çš„å™ªå£°å›¾å¯ä»¥è¯±å¯¼æ—¶é—´è¿è´¯çš„è§†é¢‘ç¼–è¾‘ç»“æœ
                - å®è·µä¸­é€šå¸¸æˆç«‹
            - ä¾èµ–äºæ‰­æ›²åœºæ˜¯ä¸€ä¸ªå¾®åˆ†åŒèƒšï¼ˆdiffeomorphismï¼‰ï¼Œè€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½¿ç”¨ç°æˆæ–¹æ³•ä¼°è®¡çš„å…‰æµå›¾å¾ˆå°‘æ˜¯å¯é€†æ˜ å°„ã€‚ï¼ˆæŠ•å½±æ˜¯ä¿¡æ¯æœ‰æŸçš„ï¼Œæ²¡æœ‰è‰¯å®šä¹‰çš„é€†æ˜ å°„ï¼‰
                - å¯¹äºå› é®æŒ¡åŒºåŸŸäº§ç”Ÿçš„**ç©ºæ´**ï¼Œæˆ‘ä»¬ç”¨æ–°é‡‡æ ·çš„å™ªå£°å€¼æ¥æ›¿æ¢è¿™äº›ç¼ºå¤±å€¼ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿å™ªå£°å›¾åœ¨è¿™äº›åŒºåŸŸä»ç„¶æ˜¯åˆç†çš„
                - åœ¨éå¸¸é•¿çš„åºåˆ—ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šå®šæœŸé‡æ–°é‡‡æ ·é«˜åˆ†è¾¨ç‡å™ªå£°ï¼Œè¿™å®é™…ä¸Šç›¸å½“äºæ›´æ–°äº†é”šå®šå¸§ã€‚è¿™æ ·åšå¯ä»¥é¿å…é•¿æ—¶é—´åºåˆ—ä¸­ç´¯ç§¯çš„è¯¯å·®ã€‚

## RLHF

### PRM 
- Deepmind PAVï¼š https://arxiv.org/pdf/2410.08146
  - åªåšäº†math
  - æœ‰æ•ˆçš„æ­¥éª¤å¥–åŠ±åº”è¯¥è¡¡é‡â€œè¿›å±•â€ï¼Œå³åœ¨æ‰§è¡ŒæŸä¸ªæ­¥éª¤ä¹‹å‰å’Œä¹‹åï¼Œäº§ç”Ÿæ­£ç¡®å“åº”çš„å¯èƒ½æ€§çš„å˜åŒ–ï¼Œæ°å¥½å¯¹åº”å¼ºåŒ–å­¦ä¹ ä¸­advantageçš„æ¦‚å¿µ
  - æ‰€ä»¥ï¼Œæ–¹æ³•å°±æ˜¯å¯¹æ¯ä¸ªstepè®¾æ³•æ ‡æ³¨ï¼ˆstate, action, advantageï¼‰ç”¨è¿™ä¸ªæ•°æ®è®­ç»ƒä¸€ä¸ªPRMï¼ˆregression-styleï¼‰
  - advantageçš„è®¡ç®—æ–¹å¼æ˜¯ï¼Œæˆ‘ä»¬å¯¹å½“å‰stateè¿›è¡Œmctsæœç´¢ï¼Œé‡‡æ ·ä¸€å †final outputï¼Œç®—å‡†ç¡®ç‡,å¯ä»¥è®¡ç®—returnï¼Œç„¶åç”¨returnè®¡ç®—Vå’ŒQï¼Œ ç”¨bellman equationè®¡ç®—advantage A=Q- V
  - ä½†æ˜¯è¿™ä¸ªpaperçš„insightæ˜¯è¯´ï¼Œè¿™ä¸ªadvantageæˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨åŸå§‹æ¨¡å‹/ç­–ç•¥è®¡ç®—(base policy, \pi), è€Œæ˜¯è¦å¼•å…¥ä¸€ä¸ªæ–°çš„prover policy(\mu), ç”¨ä»–é‡‡æ ·ï¼Œé€‰æ‹©çš„æ¡ä»¶å’Œç†ç”±ï¼š
    1. å¤šæ ·åŒ–æ•°æ®æ¥æºï¼šä¸åŒäºåŸºç¡€ç­–ç•¥çš„æ•°æ®å¯ä»¥æä¾›æ›´å¤šçš„å¤šæ ·æ€§å’Œä¸åŒçš„è§†è§’ï¼Œä»è€Œä¸°å¯Œè®­ç»ƒæ•°æ®é›†ã€‚è¿™æœ‰åŠ©äºPRMå­¦ä¼šæ›´å¹¿æ³›çš„æƒ…å†µï¼Œè€Œä¸ä»…ä»…å±€é™äºåŸºç¡€ç­–ç•¥æ‰€èƒ½è¦†ç›–çš„æƒ…å½¢ã€‚
    2. è¡¥å……ä¼˜åŠ¿ï¼šè¯æ˜è€…ç­–ç•¥ï¼ˆprover policyï¼‰é€šå¸¸æ˜¯è®¾è®¡æ¥è¡¥å……åŸºç¡€ç­–ç•¥çš„ä¸è¶³ä¹‹å¤„ã€‚å¦‚æœåŸºç¡€ç­–ç•¥åœ¨æŸäº›åœ°æ–¹ä¸å¤Ÿå¼ºï¼Œè¯æ˜è€…ç­–ç•¥å¯ä»¥å¸®åŠ©è¯†åˆ«è¿™äº›å¼±ç‚¹ï¼Œå¹¶æä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œä½¿å¾—è¿‡ç¨‹å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æ¯ä¸€æ­¥çš„å½±å“ã€‚
    3. å‡å°‘è¿‡æ‹Ÿåˆï¼šä½¿ç”¨æ¥è‡ªä¸åŒç­–ç•¥çš„æ•°æ®å¯ä»¥å‡å°‘æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡åº¦æ‹Ÿåˆåˆ°ç‰¹å®šç­–ç•¥çš„è¡Œä¸ºã€‚é€šè¿‡å¼•å…¥ä¸åŒç­–ç•¥äº§ç”Ÿçš„æ•°æ®ï¼ŒPRMå¯ä»¥å­¦ä¹ åˆ°æ›´ä¸ºæ³›åŒ–çš„æ¨¡å¼ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¼˜åŒ–ç‰¹å®šç­–ç•¥çš„è¾“å‡ºã€‚
    4. æé«˜é²æ£’æ€§ï¼šé€šè¿‡è®©PRMæ¥è§¦åˆ°ä¸åŒç­–ç•¥äº§ç”Ÿçš„è½¨è¿¹ï¼Œå¯ä»¥æé«˜å…¶é²æ£’æ€§ï¼Œä½¿å…¶åœ¨é¢å¯¹å¤šç§ä¸åŒçš„è¾“å…¥æ—¶éƒ½èƒ½ç»™å‡ºåˆç†çš„è¯„ä¼°ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°æ›´å¥½ã€‚
    5. ç†è®ºæ”¯æŒï¼šè®ºæ–‡ä¸­çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œé€‰æ‹©å¥½çš„è¯æ˜è€…ç­–ç•¥å¯ä»¥ç¡®ä¿å¯¹åŸºç¡€ç­–ç•¥è¿›è¡Œéå¹³å‡¡çš„æ”¹è¿›ã€‚å³ä½¿æ˜¯å¼±çš„è¯æ˜è€…ç­–ç•¥ä¹Ÿå¯ä»¥æ˜¾è‘—æ”¹å–„æ›´å¼ºçš„åŸºç¡€ç­–ç•¥ï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬èƒ½å¤Ÿæä¾›ä¸åŸºç¡€ç­–ç•¥ä¸åŒçš„ä¼˜åŠ¿ï¼Œä»è€Œå¸®åŠ©åŸºç¡€ç­–ç•¥æ›´å¥½åœ°å­¦ä¹ ã€‚
    6.  è¯æ˜è€… \(\mu\) æ—¢ä¸èƒ½è¿‡äºå¼ºå¤§ï¼Œä¹Ÿä¸èƒ½è¿‡äºè–„å¼±ï¼Œå¦åˆ™å®ƒæ‰€æä¾›çš„è¿‡ç¨‹å¥–åŠ±å°†æ— æ³•æœ‰æ•ˆåœ°æŒ‡å¯¼åŸºç¡€ç­–ç•¥ \(\pi\) çš„æ”¹è¿›ã€‚
	      1. å¦‚æœè¯æ˜è€… \(\mu\) ä¸åŸºç¡€ç­–ç•¥ \(\pi\) ç›¸åŒï¼Œé‚£ä¹ˆäº§ç”Ÿçš„è¿‡ç¨‹å¥–åŠ±å°†ç­‰åŒäºåªä¼˜åŒ–æœ€ç»ˆç»“æœçš„æƒ…å†µï¼Œè¿™å¯¹äºæ”¹è¿›ç­–ç•¥æ˜¯æ²¡æœ‰å¸®åŠ©çš„ã€‚
	      2. å¦‚æœè¯æ˜è€… \(\mu\) å¤ªå¼±ï¼Œåˆ™å®ƒå¯èƒ½ä¼šé¢ä¸´ä¸åŸºç¡€ç­–ç•¥ \(\pi\) ç›¸ä¼¼çš„é—®é¢˜ï¼Œå³æ— æ³•æä¾›æœ‰æ•ˆçš„åé¦ˆã€‚
	      3. ç›¸åï¼Œå¦‚æœè¯æ˜è€… \(\mu\) éå¸¸å¼ºå¤§ï¼Œé‚£ä¹ˆå³ä½¿åœ¨åŸºç¡€ç­–ç•¥ \(\pi\) æ‰§è¡Œæ— å…³ç´§è¦çš„æ­¥éª¤æ—¶ï¼Œå¼ºå¤§çš„è¯æ˜è€… \(\mu\) ä¹Ÿèƒ½æˆåŠŸåœ°ä»è¿™äº›çŠ¶æ€ä¸­æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œè¿™å¯¼è‡´è¿‡ç¨‹å¥–åŠ± \(A_{\mu}\) æ¥è¿‘äºé›¶ï¼Œå› ä¸ºå®ƒæ²¡æœ‰åŒºåˆ†å“ªäº›æ­¥éª¤æœ‰åŠ©äºè§£å†³é—®é¢˜ã€‚
	      4. å› æ­¤ï¼Œè®ºæ–‡æŒ‡å‡ºï¼Œæœ‰æ•ˆçš„è¯æ˜è€…ç­–ç•¥åº”è¯¥æ˜¯é‚£äº›èƒ½å¤Ÿè¡¥å……åŸºç¡€ç­–ç•¥ \(\pi\) çš„ç­–ç•¥ï¼Œå³èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†ç”±åŸºç¡€ç­–ç•¥äº§ç”Ÿçš„ä¸åŒæ­¥éª¤ï¼Œå¹¶æä¾›ä¸åŸºç¡€ç­–ç•¥å¯¹é½çš„æ­¥éª¤çº§ä¼˜åŠ¿ã€‚
  - å› æ­¤ï¼Œå…·ä½“å®ç°ä¸Šï¼Œç”¨BoNï¼ˆN=4ï¼‰ä½œä¸ºprover policyï¼Œç”¨prover policy sampleå‡ºæ¥çš„ç»“æœè®¡ç®—advantageï¼Œæ ‡æ³¨æ•°æ®ï¼Œè®­ç»ƒprmï¼›åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨çš„rewardæ˜¯ Q \pi + Advantage \mu (ç”¨prmé¢„æµ‹)
  - Special cases: 
    - Self-explore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards.
    - Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold.
    - Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment
- Q-ranking https://arxiv.org/pdf/2410.11287
  - åªåšäº†math
  - æ¨ç†ä¸­æ­£ç¡®çš„å’Œé”™è¯¯çš„æ­¥éª¤ä¸åŒï¼Œåº”è¯¥å¯¹åº”ä¸åŒçš„score, å› æ­¤æˆ‘ä»¬å¯ä»¥å…ˆå®šä¹‰ä¸€ç§é¡ºåºï¼Œç¬¬ä¸€æ­¥é”™< ... < ç¬¬Næ­¥é”™< ç¬¬1æ­¥å¯¹ < ... < ç¬¬Næ­¥å¯¹ï¼Œç„¶åä¼šæœ‰ä¸€å †æ’åºåˆ†æ•°ï¼Œç”¨ranking lossè®­ç»ƒprm
  - è¯æ˜è¿™ä¸ªé¡ºåºç†è®ºï¼Œç”¨çš„dpo q-functionç†è®ºï¼Œ
- MATH-Shepherd: https://arxiv.org/pdf/2312.08935
  - ç”¨cross entropyè®­ç»ƒPRM
    - HEå‡è®¾åªè¦ä¸€ä¸ªæ­¥éª¤èƒ½å¤Ÿåˆ°è¾¾æ­£ç¡®ç­”æ¡ˆï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªå¥½æ­¥éª¤ã€‚ 
    - SEåˆ™å°†æ­¥éª¤çš„è´¨é‡è§†ä¸ºå®ƒè¾¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„é¢‘ç‡ã€‚
  - used automated supervision to annotate steps with ğ‘„ values under the base policy, i.e., the PRMs score a step with the likelihood of future success, when continuing to sample from the step.
- https://arxiv.org/pdf/2410.13121ï¼Œ https://arxiv.org/pdf/2410.13246
  - ç›®å‰PRMå¾ˆéš¾æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸ
  - åœ¨å…¶ä»–é¢†åŸŸä¸Šçš„PRMï¼Œåˆ©ç”¨programs that can be executed over the scene graph object to verify each QA pair, å’Œ atomic factï¼Œæ¥åšæ¯æ­¥çš„éªŒè¯
- DPO - Q-function: https://arxiv.org/pdf/2404.12358v2
  	- under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy
- Generative Verifier https://arxiv.org/pdf/2408.15240
- PRM-version-of-BoN https://arxiv.org/pdf/2408.03314
  	- test-time beam search
- STAR
  - ç›®å‰ï¼Œè®©è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼ˆå³â€œrationalesâ€ï¼‰çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯æ„å»ºåŒ…å«æ¨ç†è¿‡ç¨‹çš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¿™ç§æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”ä¸ç°å®ï¼›å¦ä¸€ç§æ˜¯ä½¿ç”¨å°‘é‡ç¤ºä¾‹ï¼ˆfew-shotï¼‰è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä½†è¿™ç§æ–¹æ³•çš„æ€§èƒ½é€šå¸¸è¿œä½äºç›´æ¥é¢„æµ‹ç­”æ¡ˆçš„æ¨¡å‹ã€‚STaRæŠ€æœ¯é€šè¿‡è¿­ä»£åˆ©ç”¨å°‘é‡æ¨ç†ç¤ºä¾‹å’Œå¤§é‡æ— æ¨ç†æ•°æ®é›†ï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥æå‡è¿›è¡Œæ›´å¤æ‚æ¨ç†çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒSTaRæ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š
	    1. ä½¿ç”¨å°‘é‡æ¨ç†ç¤ºä¾‹å¼•å¯¼è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šä¸ªé—®é¢˜çš„æ¨ç†è¿‡ç¨‹ã€‚
	    2. å¯¹äºæ¨¡å‹ç”Ÿæˆçš„é”™è¯¯ç­”æ¡ˆï¼Œé€šè¿‡æä¾›æ­£ç¡®ç­”æ¡ˆæ¥ç”Ÿæˆæ–°çš„æ¨ç†è¿‡ç¨‹ï¼ˆç§°ä¸ºâ€œrationalizationâ€ï¼‰ã€‚
	    3. åœ¨æ‰€æœ‰æœ€ç»ˆç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¨ç†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚
	    4. é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨æ”¹è¿›åçš„æ¨¡å‹æ¥ç”Ÿæˆä¸‹ä¸€è½®çš„è®­ç»ƒæ•°æ®ã€‚
- Quiter-StAR
  - ä¿®æ”¹attn mask å¹¶è¡Œé‡‡æ · ç”Ÿæˆå¤šä¸ª<st>ï¼ˆrationaleï¼‰<et>ï¼Œ
  - å¼•å…¥â€œæ··åˆå¤´â€ï¼ˆmixing headï¼‰ï¼Œä¸€ä¸ªæµ…å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œç”¨äºç”Ÿæˆæƒé‡ï¼Œå†³å®šåœ¨ç»™å®šrationaleåï¼Œæ¨¡å‹åº”è¯¥åœ¨å¤šå¤§ç¨‹åº¦ä¸Šç»“åˆrationaleç”Ÿæˆçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¦‚ç‡å’ŒåŸºç¡€è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡ã€‚è¿™ä¸ªæ˜¯åœ¨hiddenä¸Šåšçš„ï¼Œä¸æ˜¯æ–‡æœ¬åˆ†ç±»å™¨
  - RL We thus define the reward rj for each rationale Tj as the difference between p talk j:j+ntrue and the average across rationales for that token
- V-StaR
  - ç°æœ‰çš„è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ï¼ˆå¦‚STaRï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œå¿½ç•¥äº†å¤§é‡ç”Ÿæˆçš„é”™è¯¯è§£å†³æ–¹æ¡ˆã€‚è¿™äº›é”™è¯¯è§£å†³æ–¹æ¡ˆå¯èƒ½åŒ…å«æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ å¹¶æ”¹è¿›å…¶æ¨ç†è¿‡ç¨‹ã€‚
  - é‚£ä¹ˆæ€ä¹ˆåˆ©ç”¨é”™è¯¯æ–¹æ¡ˆå‘¢ï¼Œå¯ä»¥ä½¿ç”¨DPOï¼ˆDirect Preference Optimizationï¼‰æ–¹æ³•ç›´æ¥ç”¨äºè®­ç»ƒéªŒè¯å™¨
  - combines the approach of STaR  with a DPO trained verifer. At inference time, the STaR model produces several candidate reasoning chains (plans) which are ranked by the DPO verifer likelihood
- PRM-800k https://arxiv.org/abs/2305.20050
  - ç¬¬ä¸€æ¬¡æå‡º ç»“æœç›‘ç£ï¼ˆoutcome supervisionï¼‰ORM å’Œè¿‡ç¨‹ç›‘ç£ï¼ˆprocess supervisionï¼‰PRMæ¦‚å¿µ
  - ç›´æ¥äººå·¥æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«800,000ä¸ªstep-wiseçš„äººç±»åé¦ˆæ ‡ç­¾çš„å®Œæ•´æ•°æ®é›†ï¼ˆPRM800Kï¼‰ï¼Œæ²¡æœ‰å¼€æº
- MCTS æµç¨‹ç†è§£ï¼š https://mp.weixin.qq.com/s/BrLxo_p07zX6AqmGtUoVmw

## Data-Mining

- D-CPT Law: Domain-specific Continual Pre-Training Scaling Law
- Instruction-Mining
- [domain upsample](https://arxiv.org/pdf/2406.03476) 
- LLaMA3.1 tech report æ•°æ®æ¸…ç†ç­–ç•¥ï¼š
	- Post-training
		- preference dataï¼šå¤šä¸ªæ¨¡å‹ç”Ÿæˆï¼Œè®©annotatorså»æ ‡æ³¨æˆ–è€…editï¼Œç”¨edited > chosen > rejectedæ’åº, åªé‡‡æ · chosenæ˜æ˜¾å¥½äºrejectedçš„æ•°æ®ï¼Œé˜²æ­¢æ··æ·†, å¹¶ä¸”åœ¨æ¯ä¸€è½®éƒ½ä¼šåŠ å¤§éš¾åº¦
		- SFT data: ç”¨reward modelé€‰æ‹© æœ€æ–°æ¨¡å‹çš„å¯¹è¯å›å¤ ï¼Œå¹¶åœ¨åæœŸåŠ system promptå¼•å¯¼é£æ ¼è¯­æ°”
		- data cleanï¼š identify overused phrases (such as â€œIâ€™m sorryâ€ or â€œI apologizeâ€)ï¼Œ excessive use of emojis or exclamation points
		- data pruning:  ï¼ˆtopicï¼‰llama8b ä½œä¸ºtopic classifierï¼›ï¼ˆqualityï¼‰ llama3 2/3ä¸ªçº§åˆ«çš„è´¨é‡æ‰“åˆ†ï¼Œä»¥åŠreward model å‰1/4çš„æ‰“åˆ† ä¸¤è€… **æˆ–å…³ç³»**ï¼› ï¼ˆdifficultyï¼‰Instag æ„å›¾æ•°é‡å’Œ Llama 3ä¸ªçº§åˆ«çš„æ‰“åˆ†ï¼›(semantic deduplication) RoBERTa cluster
		- average models: branch-train-mix
		- æŒ‰ç…§Llama 2çš„åšæ³•ï¼Œæˆ‘ä»¬åº”ç”¨ä¸Šè¿°æ–¹æ³•è¿›è¡Œå…­è½®è¿­ä»£ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†æ–°çš„åå¥½æ³¨é‡Šå’ŒSFTæ•°æ®ï¼Œä»æœ€æ–°æ¨¡å‹ä¸­é‡‡æ ·åˆæˆæ•°æ®ã€‚
	- Pre-training
		- data mixï¼š contains roughly 50% of tokens corresponding to general knowledge, 25% of mathematical and reasoning tokens, 17% code tokens, and 8% multilingual tokens.
		- data annealing: åœ¨å¤§æ¨¡å‹è®­ç»ƒçš„æœ€åé˜¶æ®µï¼Œç”¨é«˜è´¨é‡çš„æ•°æ®å­¦ä¹ èƒ½æé«˜æ€§èƒ½ã€‚äºæ˜¯åœ¨æœ€å40Bæ•°æ®ä¸Šï¼Œä½œè€…é€æ¸å°†å­¦ä¹ ç‡è¡°å‡åˆ°0ã€‚å¹¶ä¸”ä½œè€…å‘ç°ï¼Œæ•°æ®é€€ç«æ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥ç­›æ•°æ®
		- long-context pretraining: ç”¨6ä¸ªstage é€æ­¥å°†é•¿åº¦ä»8kæ‰©å±•åˆ°128kï¼Œå¹¶ä¸”åŠ attention maské¿å…ä¸åŒæ•°æ®ä¸²å‘³ï¼ˆå¯¹é•¿æ–‡å½±å“å¾ˆå¤§ï¼‰
		- æ•°æ®çš„å®‰å…¨æ€§å’Œè´¨é‡ï¼Œweb data curation
			1. è¿‡æ»¤å™¨ç§»é™¤å¯èƒ½å«æœ‰ä¸å®‰å…¨å†…å®¹æˆ–å¤§é‡ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰çš„ç½‘ç«™æ•°æ®ï¼Œä»¥åŠæ ¹æ®å¤šç§Metaå®‰å…¨æ ‡å‡†è¢«è¯„ä¸ºæœ‰å®³çš„åŸŸåå’Œå·²çŸ¥å«æœ‰æˆäººå†…å®¹çš„åŸŸåã€‚
			2. ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨å¤„ç†éæˆªæ–­çš„ç½‘é¡µæ–‡æ¡£ åœ¨URLã€æ–‡æ¡£å’Œè¡Œçº§åˆ«è¿›è¡Œå¤šè½®å»é‡ï¼šä¿ç•™æ¯ä¸ªURLå¯¹åº”çš„æœ€æ–°ç‰ˆæœ¬é¡µé¢ã€‚ä½¿ç”¨å…¨å±€MinHashå»é‡ï¼Œç§»é™¤è¿‘ä¼¼é‡å¤çš„æ–‡æ¡£ã€‚è¿›è¡Œç±»ä¼¼ccNetçš„lineçº§åˆ«å»é‡ï¼Œç§»é™¤åœ¨æ¯3000ä¸‡æ–‡æ¡£æ¡¶ä¸­å‡ºç°è¶…è¿‡6æ¬¡çš„line
			3. å¼€å‘å¯å‘å¼è§„åˆ™ç§»é™¤é¢å¤–çš„ä½è´¨é‡æ–‡æ¡£ã€å¼‚å¸¸å€¼å’Œé‡å¤è¿‡å¤šçš„æ–‡æ¡£ã€‚ä½¿ç”¨é‡å¤n-gramè¦†ç›–ç‡å»é™¤ç”±æ—¥å¿—æˆ–é”™è¯¯æ¶ˆæ¯ç»„æˆçš„é‡å¤å†…å®¹è¡Œï¼Œä½¿ç”¨â€œdirty wordâ€è®¡æ•°è¿‡æ»¤æœªè¢«åŸŸåé˜»æ­¢åˆ—è¡¨è¦†ç›–çš„æˆäººç½‘ç«™ï¼Œä»¥åŠä½¿ç”¨ä»¤ç‰Œåˆ†å¸ƒçš„KL divergenceè¿‡æ»¤å«æœ‰å¼‚å¸¸æ•°é‡çš„å¼‚å¸¸ä»¤ç‰Œçš„æ–‡æ¡£ã€‚
			4. **åŸºäºæ¨¡å‹çš„è´¨é‡è¿‡æ»¤**ï¼šå®éªŒæ€§åœ°åº”ç”¨å„ç§åŸºäºæ¨¡å‹çš„è´¨é‡åˆ†ç±»å™¨æ¥ç­›é€‰é«˜è´¨é‡çš„æ ‡è®°ã€‚åŒ…æ‹¬ä½¿ç”¨fasttextå¿«é€Ÿåˆ†ç±»å™¨è¯†åˆ«å¯èƒ½è¢«ç»´åŸºç™¾ç§‘å¼•ç”¨çš„æ–‡æœ¬ï¼ŒåŸºäºRobertaçš„åˆ†ç±»å™¨ï¼Œå®ƒä»¬åœ¨Llama 2é¢„æµ‹ä¸Šè¿›è¡Œè®­ç»ƒã€‚
			5. **ä»£ç å’Œæ¨ç†æ•°æ®**ï¼šç±»ä¼¼äºDeepSeek-AIç­‰ï¼Œæ„å»ºç‰¹å®šé¢†åŸŸçš„ç®¡é“æå–ä»£ç å’Œä¸æ•°å­¦ç›¸å…³çš„ç½‘é¡µã€‚ä»£ç å’Œæ¨ç†åˆ†ç±»å™¨éƒ½æ˜¯åŸºäºLlama 2æ ‡æ³¨çš„ç½‘é¡µæ•°æ®è®­ç»ƒçš„DistilledRobertaæ¨¡å‹ã€‚
			6. **å¤šè¯­è¨€æ•°æ®**ï¼šä½¿ç”¨åŸºäºfasttextçš„è¯­è¨€è¯†åˆ«æ¨¡å‹å°†æ–‡æ¡£åˆ†ç±»ä¸º176ç§è¯­è¨€ã€‚

## Init

- Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer https://thegregyang.com/
	- ä¿è¯å‚æ•°åˆå§‹åŒ–+ä¼˜åŒ–å™¨ ä½¿å¾—å…¶å’Œwidth æ— å…³, hiddenæ˜¯ä¸¤ä¸ªéƒ½æ— ç©·ï¼Œinputå’Œoutputæ˜¯åªæœ‰ä¸€ä¸ªæ— ç©·, ä¸¤ä¸ªéƒ½æ— ç©·ç”¨çš„ä¸­å¿ƒæé™ï¼Œä¸€ä¸ªæ— ç©·ç”¨çš„å¤§æ•°å®šå¾‹
   	- ÂµP èƒ½å¤Ÿç¡®ä¿åœ¨æ¨¡å‹å¤§å°å˜åŒ–æ—¶ï¼Œè®¸å¤šæœ€ä¼˜è¶…å‚æ•°ä¿æŒç¨³å®šã€‚è¿™ä½¿å¾—å¯ä»¥ä»è¾ƒå°æ¨¡å‹é—´æ¥è°ƒä¼˜è¶…å‚æ•°ï¼Œç„¶åé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰è¿ç§»åˆ°å…¨å°ºå¯¸æ¨¡å‹ä¸Š
  

## LoRA

- æ¶æ„
	- VeRA: freezes random weight tied adapters and learns vector scalings of the internal adapter activations.
	- LoRA-XS: initializes the A and B matrices using the SVD of the pretrained weights and trains a low-rank update of the form BRA where R is a trainable r Ã— r matrix and B, A are fixed.
	- NOLA: parametrizes the adapter matrices to be linear combinations of frozen random matrices and optimizes the linear coefficients of the mixtures.
	- VB-LORA: shares adapter parameters using a global vector bank.
	- MoRA: learns high-rank updates while still preserving parameter efficiency by applying hand-designed compress and decompress operations before and after a trainable adapter matrix.
	- DoRA: decomposes the pretrained weight into magnitude and direction components to allow for better training dynamics
	- GaLoRe: ä½¿ç”¨SVDå°†å…¨å‚æ•°è®­ç»ƒçš„æ¢¯åº¦æŠ•å½±åˆ°ä½ç§©ç©ºé—´
	- IA3ï¼ˆImplicit Activation Scalingï¼‰: é€šè¿‡ä¿®æ”¹æ¿€æ´»å‘é‡çš„ç¼©æ”¾æ¥é€‚åº”æ¨¡å‹ï¼Œè€Œä¸æ˜¯è°ƒæ•´æƒé‡ã€‚
- è®­ç»ƒæ”¹è¿›
	- LoRA-FA: freezes the A matrix which leads to small performance loss while reducing memory consumption
	  by up to 1.4Ã—.  
	- https://arxiv.org/pdf/2406.08447v1 [initA] > [initB] é€šè¿‡å¯¹ç¥ç»ç½‘ç»œå®½åº¦æé™çš„ç†è®ºåˆ†æï¼ˆuPï¼‰
	- LoRA+: åŒæ ·ç ”ç©¶æ— é™å®½åº¦ä¸‹çš„åˆå§‹åŒ–ï¼Œç»“è®ºæ˜¯ç»™ABä¸åŒçš„å­¦ä¹ ç‡
	- Pissaï¼šå¯¹W0åšSVDæ¥åˆå§‹åŒ–A,B
	- LoRA-GAï¼šå°½é‡å¯¹é½ç¬¬ä¸€æ­¥æ›´æ–°åçš„W1ï¼Œå¯¹åˆå§‹æ¢¯åº¦G0=âˆ‡W0LåšSVDï¼Œå–Uçš„å‰råˆ—åˆå§‹åŒ–Aï¼Œå–Vçš„ç¬¬r+1âˆ¼2rè¡Œåˆå§‹åŒ–B
	- **LoRA-Pro**: å¯¹é½å…¨é‡å¾®è°ƒå’ŒLoRAçš„æ¯ä¸€ä¸ªWt,
- æ•ˆæœ
	- https://arxiv.org/pdf/2405.09673 LoRAåœ¨ç›®æ ‡é¢†åŸŸçš„æ€§èƒ½é€šå¸¸ä½äºå…¨å‚æ•°å¾®è°ƒï¼Œä½†åœ¨ä¿æŒæºé¢†åŸŸæ€§èƒ½æ–¹é¢è¡¨ç°æ›´å¥½ï¼›LoRAæä¾›äº†æ¯”ä¼ ç»Ÿæ­£åˆ™åŒ–æŠ€æœ¯(finetuned, weight-decay)æ›´å¼ºçš„æ­£åˆ™åŒ–æ•ˆæœï¼Œå¹¶æœ‰åŠ©äºä¿æŒç”Ÿæˆå¤šæ ·æ€§
  - QLoRA: matched full finetuning MMLU (Hendrycks et al., 2020) performance, optimized LoRA configurations perform as well as full finetuning, and that performance is governed by choice of target modules but not rank.
  - DoRA: shows that LoRA is sensitive to ranks. It is likely that some of these discrepancies
    are due to differences in finetuning datasets and evaluations.  
- é¢„è®­ç»ƒ
	- SwitchLoRAï¼šæ„é€ min(m,n)ä¸ªå€™é€‰çš„è¡Œå‘é‡å’Œåˆ—å‘é‡ï¼Œç„¶åæ¯æ­¥éšæœºå–ä¸€ä¸ªæ’åˆ°Aå’ŒBä¸Šè®­ç»ƒï¼Œä¿è¯è®­ç»ƒæ»¡ç§©
## Decoding


- Learning to Decode Collaboratively with Multiple Language Models [[paper]](https://arxiv.org/abs/2403.03870)


- Survey: Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding [[paper]](https://arxiv.org/abs/2401.07851)
- Fast Inference from Transformers via Speculative Decoding [[paper]](https://arxiv.org/pdf/2211.17192)[[repo]](https://github.com/feifeibear/LLMSpeculativeSampling) 
- Accelerating Large Language Model Decoding with Speculative Sampling [[paper]](https://arxiv.org/pdf/2302.01318)
- [ASPLOS'24] SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification [[paper]](https://arxiv.org/abs/2305.09781)
- [ICML24] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [[paper]](https://arxiv.org/pdf/2401.15077) [[blog]](https://sites.google.com/view/eagle-llm)
- EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees [[paper]](https://arxiv.org/pdf/2406.16858)
- [ICLR24] DistillSpec: Improving Speculative Decoding via Knowledge Distillation [[paper]](https://arxiv.org/abs/2310.08461) ç”¨target modelä½œä¸ºteacherå¯¹draft modelè’¸é¦
- [NAACL24] REST: Retrieval-Based Speculative Decoding [[paper]](https://arxiv.org/pdf/2311.08252)
- Graph-Structured Speculative Decoding [[paper]](https://arxiv.org/pdf/2407.16207)

## Long-Context


- ç›¸å¯¹åç½®
	- ALIBI [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409), ICLR22
	  > åœ¨Softmaxä¹‹å‰å‡å»ä¸€ä¸ªéè´ŸçŸ©é˜µï¼Œå°†Attentionçš„è®¡ç®—ä»$q_{m} k_{n}$æ”¹ä¸º $q_{m} k_{n}-\lambda|m-n|$ï¼Œå…¶ä¸­Î»>0æ˜¯è¶…å‚æ•°ï¼Œæ¯ä¸ªheadè®¾ç½®ä¸åŒçš„å€¼
- base ç¼©æ”¾
	- Position Interpolation (PI) [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595)
	  > ä½ç½®ç¼–ç baseï¼ˆé»˜è®¤ä¸º10000ï¼‰ä¹˜ä¸Šå› å­$L_{train}/L_{test}$
	- Dynamic-NTK  [Reddit](www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
	  > PIçš„ç¼©æ”¾æ˜¯å¹³ç­‰å¯¹å¾…åœ°å¯¹å¾…æ‰€æœ‰ç»´åº¦ï¼Œå³é«˜é¢‘æ—‹è½¬è§’åº¦ç¼©å°çš„å€æ•°å’Œä½é¢‘æ—‹è½¬è§’åº¦ç¼©å°çš„å€æ•°æ˜¯ä¸€æ ·çš„ã€‚ NTK-Aware Scaled RoPE å¯ä»¥ç†è§£ä¸ºå¯¹ä½é¢‘å†…æ’ï¼Œé«˜é¢‘å¤–æ’
	- [YaRN: Efficient Context Window Extension of Large Language Models](https://openreview.net/forum?id=wHBfxhZu1u), ICLR 2024
	  > 1. å¦‚æœç»´åº¦iå¯¹åº”çš„æ³¢é•¿$$\lambda_i$$è¿œå°äºæ–‡æœ¬é•¿åº¦ï¼Œä¸è¿›è¡Œå†…æ’åªå¤–æ¨ 2. å¦‚æœç»´åº¦iå¯¹åº”çš„æ³¢é•¿$$\lambda_i$$å¤§äºæ–‡æœ¬é•¿åº¦ï¼Œè¿›è¡Œå†…æ’ 3. å¯¹äºä¸­é—´éƒ¨åˆ†ï¼Œé‡‡ç”¨NTK-Aware Scaled RoPEçš„æ€è·¯
	- [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://openreview.net/forum?id=ONOtpXLqqw), ICML 2024
	  > RoPE çš„ä¸åŒç»´åº¦å­˜åœ¨ä¸å‡è¡¡æ€§, ç”¨è¿›åŒ–ç®—æ³•æœç´¢éå‡åŒ€ä½ç½®æ’å€¼
- åˆ†å— / chunk
	- MRC / Long Text Match é‡Œè¾¹æœ‰å¾ˆå¤š
	- [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
	  >  ä»¥ä¸€ä¸ªå›ºå®šé•¿åº¦åˆ’åˆ†å¤šä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£å†…éƒ¨åš Attention, æŠŠåˆ†çª—å£æ³¨æ„åŠ›çš„ Attention mask å‘ä¸‹ç§»åŠ¨åŠä¸ªçª—å£çš„é•¿åº¦ï¼Œè®©ä¸Šé¢æåˆ°çš„ä¸åŒçš„çª—å£ä¹‹é—´è¿›è¡Œäº¤äº’; å¹¶ä¸”LORAå¢åŠ å¯¹LayerNorm å’Œ Embedding çš„å¾®è°ƒ
	- [LongHeads: Multi-Head Attention is Secretly a Long Context Processor](https://arxiv.org/abs/2402.10685)
	  > è®¤ä¸ºä¸åŒçš„æ³¨æ„åŠ›å¤´æ‰€å…³æ³¨çš„æ˜¯ context ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œå°†é•¿ context åˆ†è§£æˆå¤šä¸ªå—ï¼Œè®©æ¯ä¸ªæ³¨æ„åŠ›å¤´åˆ†åˆ«å…³æ³¨é‡è¦çš„å—ï¼Œå¹¶ä¿è¯åˆ†é…ç»™æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é‚£äº›å—åŒ…å«çš„ tokens æ•°å°‘äºé¢„è®­ç»ƒçš„çª—å£å¤§å°
	- [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](https://openreview.net/forum?id=ulaUJFd96G), ICLR24
	  >  å°†é•¿æ–‡æœ¬è¾“å…¥åˆ†å‰²æˆå¤šä¸ªå°å—ï¼ˆchunksï¼‰, å±‚æ¬¡åŒ–åˆå¹¶ç­–ç•¥ï¼Œé€šè¿‡åœ¨ä¸åŒçš„transformerå±‚çº§é€æ­¥åˆå¹¶ç›¸é‚»çš„å—ï¼Œä½¿å¾—ä¿¡æ¯å¯ä»¥åœ¨å—ä¹‹é—´ä¼ é€’ã€‚
	- [Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/abs/2402.17463), ICML 2024
	  >  æŒ‰å—è¿›è¡Œæ—‹è½¬ç¼–ç ï¼ˆRoPEï¼‰ï¼Œæå‡ºä¸€ç§å—å†…ï¼Œå—é—´ä»¥åŠç›¸é‚»å—Attentionçš„DCAç­–ç•¥
	- [LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning](https://openreview.net/forum?id=nkOMLBIiI7), ICML2024
	  > åˆ©ç”¨ â€œgrouped attentionâ€  è·ç¦»è¿‘çš„ç”¨åŸæ¥çš„attentionï¼Œè¿œçš„è¯ç”¨grouped attention


## KV-Cache

- Survey: https://github.com/October2001/Awesome-KV-Cache-Compression
- KV Cache Quantization
	- Coupled Quantization (Zhang et al., 2024b). and KIVI (Zirui Liu et al., 2023), have demonstrated that KV cache can be quantized to 1-bit or 2-bit precision while preserving performance.
	- [IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact](https://arxiv.org/abs/2403.01241)
	  > Pivot tokens ä½œä¸ºé‡è¦çš„å‰ç¼€ï¼Œä¸åº”è¯¥è¿›è¡Œé‡åŒ–  
- KV Cache Low-Rank
	- insights: è®¤ä¸ºKV cacheæ˜¯ä½ç§©çš„
	- [Effectively Compress KV Heads for LLM](https://arxiv.org/abs/2406.07056)
	  > (1) only 25% of the highest singular values need to be retained to get most of the energy.   
	  (2) RoPE generally reduces the rank of key cache  
	  éœ€è¦æ•°æ®å¹¶ä¸”åœ¨æ¿€æ´»å€¼ä¸Šè¿›è¡ŒSVDçš„æ–¹æ³•ç§°ä¸ºSVD-aï¼Œç›´æ¥åœ¨æƒé‡çŸ©é˜µä¸ŠåšSVDç§°ä¸ºSVD-w  
	- LESS: [Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2402.09398)
	  > ç”¨ä½ç§©çŸ©é˜µæ¥æ‹Ÿåˆ kv cache token dropping å¸¦æ¥çš„è¯¯å·®ï¼Œattention map çš„è¯¯å·®çŸ©é˜µå¾€å¾€æ˜¯ä½ç§©çš„ï¼Œå› æ­¤å¯ä»¥ç±»ä¼¼ Linear Attention çš„åšæ³•ï¼ŒæŠŠ softmax ä¸­è¡¨ç¤ºç›¸ä¼¼åº¦çš„æŒ‡æ•°éƒ¨åˆ†ï¼Œæ›´æ¢ä¸ºåˆ†åˆ«å¯¹ qk è¿›è¡Œå˜æ¢ç„¶ååšå†…ç§¯  
	- [GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527)
	  > ç”¨ä½ç§©çŸ©é˜µæ‹Ÿåˆkv cacheé‡åŒ–å¸¦æ¥çš„è¯¯å·®ï¼Œå› ä¸ºé‡åŒ–å¯¼è‡´çš„è¯¯å·®çŸ©é˜µç§©æ¯”è¾ƒä½ï¼Œå¯ä»¥ç”¨ä¸¤ä¸ªçŸ©é˜µæ‹Ÿåˆ  
- KV Cache Eviction
	- insights: attentionæœ¬èº«å…·æœ‰çš„ç¨€ç–æ€§, 50%çš„ KV cacheè´¡çŒ®äº†0.9ä»¥ä¸Šçš„ Attention Scores
	- Scissorhands (Liu et al., 2023b)
	  keeps a fixed KV size budget and replies on the Persistence of Importance hypothesis to evict key  
	  and value states for non-important tokens  
	- [H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf)ï¼ŒNeurIPS 2023
	  > 20%ç¼“å­˜æ¥è¿‘äºå…¨é‡KV cacheçš„æ•ˆæœ;   
	  utilizes aggregated attention scores to determine so called â€œheavy hittersâ€/ important tokens  
	- StreamingLLM [Efficient Streaming Language Models with Attention Sinks](http://arxiv.org/abs/2309.17453), ICLR 2024
	  > attention sink + the recent window tokens is pivotal to maintain LLMâ€™s performance  
	- FastGen [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://arxiv.org/abs/2310.01801)
	> attention sinks also occurs in the middle of the sentences
	è®¤ä¸ºæ³¨æ„åŠ›å¤´åœ¨ä¸åŒä½ç½®ä¸‹çš„ attention map ç»“æ„æ˜¯ç›¸å¯¹ç¨³å®šçš„ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡è¾“å…¥çš„ prompt æ¥ç¡®å®šæ³¨æ„åŠ›å¤´çš„å…¨å±€æ¨¡å¼
	- VATP [Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters](https://arxiv.org/abs/2406.12335)
	  > attention sink tokens å¯¹åº”çš„å€¼å‘é‡çš„ $l_1$ normæ¯”å…¶ä»–å°å¾ˆå¤š, åŸºäºattention score å’Œå€¼å‘é‡çš„L1èŒƒæ•°çš„ä¹˜ç§¯æ¥æŒ‘é€‰KV cache  
	- [Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference](https://arxiv.org/pdf/2403.09054) 
	  > H2O æ”¹è¿› ä½¿ç”¨Gumbelåˆ†å¸ƒå¼•å…¥å™ªå£°ä»¥è°ƒæ•´æœªå½’ä¸€åŒ–logitsï¼Œä»è€Œè§£å†³ç”±äºä¸¢å¼ƒtokenè€Œå¯¼è‡´çš„uneven score distribution é—®é¢˜ã€‚  
- KV Cache Merging ï¼ˆToken Merging, Token Pooling, Token Pruningï¼‰
	- insightsï¼š
		- directly eviction may accidentally and permanently remove important tokens;
		- key states exhibit high similarity at the token level within a single sequence
	- token merging is well-established in computer vision (CV)
	  (Zeng et al., 2022) (Bolya et al., 2023) (Kim et al., 2023) (Zhang et al., 2024a),  
	- [CaM: Cache Merging for Memory-efficient LLMs Inference](https://openreview.net/pdf?id=LCTmppB165), ICML 2024. 
	  > ä¸æ˜¯ç›´æ¥å°†å…¶éœ€è¦é€å‡ºçš„tokenä¸¢å¼ƒï¼Œè€Œæ˜¯é€šè¿‡mergeæ¥åˆ©ç”¨é€å‡ºçš„å…ƒç´   
	  paperé‡Œè¾¹ç†è®ºè¯æ˜å¥½å¤„åœ¨äºå¯¹attentionçš„è¾“å‡ºæ‰°åŠ¨æ›´å°ã€‚  
	- [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636), ICML 2024
	  > å°†æ¯æ¬¡æ–°è¿›å…¥çš„KV mergeï¼Œ å¯¹äºæ¯ä¸ªæ–°æ¥çš„kvï¼Œå†³å®šæ˜¯mergeè¿˜æ˜¯append  
	- [LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference](https://openreview.net/pdf/d775ca7f5d0bfad0e56d5e710a3953555ccaabda.pdf)
	  > åº”ç”¨åœ¨MLLMä¸Š (è¾“å…¥æ˜¯interleavedçš„å›¾æ–‡å¯¹)ï¼Œæ–‡æœ¬ä¸æ“ä½œï¼Œå¯¹å›¾åƒtoken merge å› ä¸ºåœ¨å¤šæ¨¡æ€ä¸­æ–‡æœ¬çš„ç›¸å¯¹å›¾ç‰‡å…·æœ‰æ›´é«˜çš„attention scoreï¼Œæå‡º4ç§mergeç­–ç•¥ï¼šMaxï¼ŒMeanï¼ŒPivotal ï¼Œweighted  
	- [KVMerger: Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks](https://arxiv.org/abs/2407.08454)
	  > è®¤ä¸ºä¹‹å‰çš„å·¥ä½œåœ¨identity merge setä¸Šå­˜åœ¨ç¼ºé™·ï¼Œå‘ç°KV cacheçš„å‹ç¼©æ¯”ç‡åœ¨ä¸åŒæ ·æœ¬ä¸Šé«˜åº¦ä¸€è‡´(æ¨¡å‹å›ºæœ‰ç‰¹æ€§)ï¼Œå› æ­¤å¯ä»¥ç›´æ¥ç”¨layer-wiseçš„cos-simé™æ€è®¡ç®—å‹ç¼©æ¯”ç‡ï¼›åœ¨LLMsçš„å‰ä¸¤å±‚å’Œæœ€åä¸€å±‚æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒæ›´åŠ å‡åŒ€ï¼Œæ„å‘³ç€å¤§å¤šæ•°é”®çŠ¶æ€éƒ½å¾ˆé‡è¦ï¼Œåº”è¯¥ä¸mergingä»¥é¿å…å¼•å…¥æ˜¾è‘—çš„å™ªå£°ï¼› Gaussian kernel weighted merging algorithm  
- KV Cache CrossAttn
	- CLA [Reducing Transformer Key-Value Cache Size with Cross-Layer Attention](https://arxiv.org/abs/2405.12981)
	  > ç›¸é‚»layer å…±äº«KV cache  

## Model-Merging

- survey: https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications

## Image-Tokenizer

## Interpretable

- [ACL25] https://arxiv.org/pdf/2410.23743  æ­£æ–‡8é¡µä½†æ˜¯é™„å½•æœ‰123é¡µ
	- **Nuclear Norm** (the â„“1 norm of singular values) to represent the characteristics for the gradient of each layer $$s_{X,i} = \sum |\sigma_j|, X \in \{QKVO\}, i \in [0, N-1]$$ , N is layernum, QKVO is attention matrix
	- **mean absolute differences (MAD)** is defined as $$\mathrm{MAD}_{s_X}=\frac{1}{N-1} \sum_{i=1}^{N-1}\left|s_{X, i+1}-s_{X, i}\right|$$
	- None CoT / Simplified CoT / Detailed CoT (Slow vs. Fast Thinking)
		 - The large scale of MAD indicates that the response distributions that LLMs are going to learn have large discrepancies with what it has learned from the pretraining phase, which might harm the performances of the original pre-trained models
		- A consistent decrease in MAD is observed in all layers when LLMs are trained to produce more detailed reasoning paths (slow thinking)
		- LLMs can to some extent identify that the responses to be learned have potential conflicts
		  with their internal knowledge, thus requiring more energy to adapt to the new nonsense responses.


## Squence Parallel
- data/tensor/zero/expert/pipeline parallelism.
- Sequence Parallelism of Megatron-LM:  this form of Sequence Parallelism cannot be used independently without tensor parallelism
- [Deepspeed-Ulysses](https://arxiv.org/abs/2309.14509) P2P communication
- [Ring-Attention](https://arxiv.org/pdf/2310.01889) a distributed version of FlashAttention, All2All communication;
	- Context Parallel https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html
- [USP-Attention](https://arxiv.org/pdf/2405.07719) 

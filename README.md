# Paper-Reading

## RLHF

### PRM 
- Deepmind PAVï¼š https://arxiv.org/pdf/2410.08146
  - åªåšäº†math
  - æœ‰æ•ˆçš„æ­¥éª¤å¥–åŠ±åº”è¯¥è¡¡é‡â€œè¿›å±•â€ï¼Œå³åœ¨æ‰§è¡ŒæŸä¸ªæ­¥éª¤ä¹‹å‰å’Œä¹‹åï¼Œäº§ç”Ÿæ­£ç¡®å“åº”çš„å¯èƒ½æ€§çš„å˜åŒ–ï¼Œæ°å¥½å¯¹åº”å¼ºåŒ–å­¦ä¹ ä¸­advantageçš„æ¦‚å¿µ
  - æ‰€ä»¥ï¼Œæ–¹æ³•å°±æ˜¯å¯¹æ¯ä¸ªstepè®¾æ³•æ ‡æ³¨ï¼ˆstate, action, advantageï¼‰ç”¨è¿™ä¸ªæ•°æ®è®­ç»ƒä¸€ä¸ªPRMï¼ˆregression-styleï¼‰
  - advantageçš„è®¡ç®—æ–¹å¼æ˜¯ï¼Œæˆ‘ä»¬å¯¹å½“å‰stateè¿›è¡Œmctsæœç´¢ï¼Œé‡‡æ ·ä¸€å †final outputï¼Œç®—å‡†ç¡®ç‡,å¯ä»¥è®¡ç®—returnï¼Œç„¶åç”¨returnè®¡ç®—Vå’ŒQï¼Œ ç”¨bellman equationè®¡ç®—advantage A=Q- V
  - ä½†æ˜¯è¿™ä¸ªpaperçš„insightæ˜¯è¯´ï¼Œè¿™ä¸ªadvantageæˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨åŸå§‹æ¨¡å‹/ç­–ç•¥è®¡ç®—(base policy, \pi), è€Œæ˜¯è¦å¼•å…¥ä¸€ä¸ªæ–°çš„prover policy(\mu), ç”¨ä»–é‡‡æ ·ï¼Œé€‰æ‹©çš„æ¡ä»¶å’Œç†ç”±ï¼š
    1. å¤šæ ·åŒ–æ•°æ®æ¥æºï¼šä¸åŒäºåŸºç¡€ç­–ç•¥çš„æ•°æ®å¯ä»¥æä¾›æ›´å¤šçš„å¤šæ ·æ€§å’Œä¸åŒçš„è§†è§’ï¼Œä»è€Œä¸°å¯Œè®­ç»ƒæ•°æ®é›†ã€‚è¿™æœ‰åŠ©äºPRMå­¦ä¼šæ›´å¹¿æ³›çš„æƒ…å†µï¼Œè€Œä¸ä»…ä»…å±€é™äºåŸºç¡€ç­–ç•¥æ‰€èƒ½è¦†ç›–çš„æƒ…å½¢ã€‚
    2. è¡¥å……ä¼˜åŠ¿ï¼šè¯æ˜è€…ç­–ç•¥ï¼ˆprover policyï¼‰é€šå¸¸æ˜¯è®¾è®¡æ¥è¡¥å……åŸºç¡€ç­–ç•¥çš„ä¸è¶³ä¹‹å¤„ã€‚å¦‚æœåŸºç¡€ç­–ç•¥åœ¨æŸäº›åœ°æ–¹ä¸å¤Ÿå¼ºï¼Œè¯æ˜è€…ç­–ç•¥å¯ä»¥å¸®åŠ©è¯†åˆ«è¿™äº›å¼±ç‚¹ï¼Œå¹¶æä¾›é¢å¤–çš„ä¿¡æ¯ï¼Œä½¿å¾—è¿‡ç¨‹å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æ¯ä¸€æ­¥çš„å½±å“ã€‚
    3. å‡å°‘è¿‡æ‹Ÿåˆï¼šä½¿ç”¨æ¥è‡ªä¸åŒç­–ç•¥çš„æ•°æ®å¯ä»¥å‡å°‘æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿‡åº¦æ‹Ÿåˆåˆ°ç‰¹å®šç­–ç•¥çš„è¡Œä¸ºã€‚é€šè¿‡å¼•å…¥ä¸åŒç­–ç•¥äº§ç”Ÿçš„æ•°æ®ï¼ŒPRMå¯ä»¥å­¦ä¹ åˆ°æ›´ä¸ºæ³›åŒ–çš„æ¨¡å¼ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¼˜åŒ–ç‰¹å®šç­–ç•¥çš„è¾“å‡ºã€‚
    4. æé«˜é²æ£’æ€§ï¼šé€šè¿‡è®©PRMæ¥è§¦åˆ°ä¸åŒç­–ç•¥äº§ç”Ÿçš„è½¨è¿¹ï¼Œå¯ä»¥æé«˜å…¶é²æ£’æ€§ï¼Œä½¿å…¶åœ¨é¢å¯¹å¤šç§ä¸åŒçš„è¾“å…¥æ—¶éƒ½èƒ½ç»™å‡ºåˆç†çš„è¯„ä¼°ï¼Œä»è€Œåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°æ›´å¥½ã€‚
    5. ç†è®ºæ”¯æŒï¼šè®ºæ–‡ä¸­çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œé€‰æ‹©å¥½çš„è¯æ˜è€…ç­–ç•¥å¯ä»¥ç¡®ä¿å¯¹åŸºç¡€ç­–ç•¥è¿›è¡Œéå¹³å‡¡çš„æ”¹è¿›ã€‚å³ä½¿æ˜¯å¼±çš„è¯æ˜è€…ç­–ç•¥ä¹Ÿå¯ä»¥æ˜¾è‘—æ”¹å–„æ›´å¼ºçš„åŸºç¡€ç­–ç•¥ï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬èƒ½å¤Ÿæä¾›ä¸åŸºç¡€ç­–ç•¥ä¸åŒçš„ä¼˜åŠ¿ï¼Œä»è€Œå¸®åŠ©åŸºç¡€ç­–ç•¥æ›´å¥½åœ°å­¦ä¹ ã€‚
    6.  è¯æ˜è€… \(\mu\) æ—¢ä¸èƒ½è¿‡äºå¼ºå¤§ï¼Œä¹Ÿä¸èƒ½è¿‡äºè–„å¼±ï¼Œå¦åˆ™å®ƒæ‰€æä¾›çš„è¿‡ç¨‹å¥–åŠ±å°†æ— æ³•æœ‰æ•ˆåœ°æŒ‡å¯¼åŸºç¡€ç­–ç•¥ \(\pi\) çš„æ”¹è¿›ã€‚
	      1. å¦‚æœè¯æ˜è€… \(\mu\) ä¸åŸºç¡€ç­–ç•¥ \(\pi\) ç›¸åŒï¼Œé‚£ä¹ˆäº§ç”Ÿçš„è¿‡ç¨‹å¥–åŠ±å°†ç­‰åŒäºåªä¼˜åŒ–æœ€ç»ˆç»“æœçš„æƒ…å†µï¼Œè¿™å¯¹äºæ”¹è¿›ç­–ç•¥æ˜¯æ²¡æœ‰å¸®åŠ©çš„ã€‚
	      2. å¦‚æœè¯æ˜è€… \(\mu\) å¤ªå¼±ï¼Œåˆ™å®ƒå¯èƒ½ä¼šé¢ä¸´ä¸åŸºç¡€ç­–ç•¥ \(\pi\) ç›¸ä¼¼çš„é—®é¢˜ï¼Œå³æ— æ³•æä¾›æœ‰æ•ˆçš„åé¦ˆã€‚
	      3. ç›¸åï¼Œå¦‚æœè¯æ˜è€… \(\mu\) éå¸¸å¼ºå¤§ï¼Œé‚£ä¹ˆå³ä½¿åœ¨åŸºç¡€ç­–ç•¥ \(\pi\) æ‰§è¡Œæ— å…³ç´§è¦çš„æ­¥éª¤æ—¶ï¼Œå¼ºå¤§çš„è¯æ˜è€… \(\mu\) ä¹Ÿèƒ½æˆåŠŸåœ°ä»è¿™äº›çŠ¶æ€ä¸­æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œè¿™å¯¼è‡´è¿‡ç¨‹å¥–åŠ± \(A_{\mu}\) æ¥è¿‘äºé›¶ï¼Œå› ä¸ºå®ƒæ²¡æœ‰åŒºåˆ†å“ªäº›æ­¥éª¤æœ‰åŠ©äºè§£å†³é—®é¢˜ã€‚
	      4. å› æ­¤ï¼Œè®ºæ–‡æŒ‡å‡ºï¼Œæœ‰æ•ˆçš„è¯æ˜è€…ç­–ç•¥åº”è¯¥æ˜¯é‚£äº›èƒ½å¤Ÿè¡¥å……åŸºç¡€ç­–ç•¥ \(\pi\) çš„ç­–ç•¥ï¼Œå³èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†ç”±åŸºç¡€ç­–ç•¥äº§ç”Ÿçš„ä¸åŒæ­¥éª¤ï¼Œå¹¶æä¾›ä¸åŸºç¡€ç­–ç•¥å¯¹é½çš„æ­¥éª¤çº§ä¼˜åŠ¿ã€‚
  - å› æ­¤ï¼Œå…·ä½“å®ç°ä¸Šï¼Œç”¨BoNï¼ˆN=4ï¼‰ä½œä¸ºprover policyï¼Œç”¨prover policy sampleå‡ºæ¥çš„ç»“æœè®¡ç®—advantageï¼Œæ ‡æ³¨æ•°æ®ï¼Œè®­ç»ƒprmï¼›åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨çš„rewardæ˜¯ Q \pi + Advantage \mu (ç”¨prmé¢„æµ‹)
  - Special cases: 
    - Self-explore to avoid the pit: Improving the reasoning capabilities of language models with fine-grained rewards.
    - Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold.
    - Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment
- Q-ranking https://arxiv.org/pdf/2410.11287
  - åªåšäº†math
  - æ¨ç†ä¸­æ­£ç¡®çš„å’Œé”™è¯¯çš„æ­¥éª¤ä¸åŒï¼Œåº”è¯¥å¯¹åº”ä¸åŒçš„score, å› æ­¤æˆ‘ä»¬å¯ä»¥å…ˆå®šä¹‰ä¸€ç§é¡ºåºï¼Œç¬¬ä¸€æ­¥é”™< ... < ç¬¬Næ­¥é”™< ç¬¬1æ­¥å¯¹ < ... < ç¬¬Næ­¥å¯¹ï¼Œç„¶åä¼šæœ‰ä¸€å †æ’åºåˆ†æ•°ï¼Œç”¨ranking lossè®­ç»ƒprm
  - è¯æ˜è¿™ä¸ªé¡ºåºç†è®ºï¼Œç”¨çš„dpo q-functionç†è®ºï¼Œ
- MATH-Shepherd: https://arxiv.org/pdf/2312.08935
  - ç”¨cross entropyè®­ç»ƒPRM
    - HEå‡è®¾åªè¦ä¸€ä¸ªæ­¥éª¤èƒ½å¤Ÿåˆ°è¾¾æ­£ç¡®ç­”æ¡ˆï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªå¥½æ­¥éª¤ã€‚ 
    - SEåˆ™å°†æ­¥éª¤çš„è´¨é‡è§†ä¸ºå®ƒè¾¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„é¢‘ç‡ã€‚
  - used automated supervision to annotate steps with ğ‘„ values under the base policy, i.e., the PRMs score a step with the likelihood of future success, when continuing to sample from the step.
- https://arxiv.org/pdf/2410.13121ï¼Œ https://arxiv.org/pdf/2410.13246
  - ç›®å‰PRMå¾ˆéš¾æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸ
  - åœ¨å…¶ä»–é¢†åŸŸä¸Šçš„PRMï¼Œåˆ©ç”¨programs that can be executed over the scene graph object to verify each QA pair, å’Œ atomic factï¼Œæ¥åšæ¯æ­¥çš„éªŒè¯
- DPO - Q-function: https://arxiv.org/pdf/2404.12358v2
  	- under the token level formulation, classical search-based algorithms, such as MCTS, which have recently been applied to the language generation space, are equivalent to likelihood-based search on a DPO policy
- Generative Verifier https://arxiv.org/pdf/2408.15240
- PRM-version-of-BoN https://arxiv.org/pdf/2408.03314
  	- test-time beam search
- STAR
  - ç›®å‰ï¼Œè®©è¯­è¨€æ¨¡å‹ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼ˆå³â€œrationalesâ€ï¼‰çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šä¸€ç§æ˜¯æ„å»ºåŒ…å«æ¨ç†è¿‡ç¨‹çš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œè¿™ç§æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”ä¸ç°å®ï¼›å¦ä¸€ç§æ˜¯ä½¿ç”¨å°‘é‡ç¤ºä¾‹ï¼ˆfew-shotï¼‰è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä½†è¿™ç§æ–¹æ³•çš„æ€§èƒ½é€šå¸¸è¿œä½äºç›´æ¥é¢„æµ‹ç­”æ¡ˆçš„æ¨¡å‹ã€‚STaRæŠ€æœ¯é€šè¿‡è¿­ä»£åˆ©ç”¨å°‘é‡æ¨ç†ç¤ºä¾‹å’Œå¤§é‡æ— æ¨ç†æ•°æ®é›†ï¼Œå¼•å¯¼æ¨¡å‹é€æ­¥æå‡è¿›è¡Œæ›´å¤æ‚æ¨ç†çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒSTaRæ–¹æ³•åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š
	    1. ä½¿ç”¨å°‘é‡æ¨ç†ç¤ºä¾‹å¼•å¯¼è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šä¸ªé—®é¢˜çš„æ¨ç†è¿‡ç¨‹ã€‚
	    2. å¯¹äºæ¨¡å‹ç”Ÿæˆçš„é”™è¯¯ç­”æ¡ˆï¼Œé€šè¿‡æä¾›æ­£ç¡®ç­”æ¡ˆæ¥ç”Ÿæˆæ–°çš„æ¨ç†è¿‡ç¨‹ï¼ˆç§°ä¸ºâ€œrationalizationâ€ï¼‰ã€‚
	    3. åœ¨æ‰€æœ‰æœ€ç»ˆç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„æ¨ç†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚
	    4. é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨æ”¹è¿›åçš„æ¨¡å‹æ¥ç”Ÿæˆä¸‹ä¸€è½®çš„è®­ç»ƒæ•°æ®ã€‚
- Quiter-StAR
  - ä¿®æ”¹attn mask å¹¶è¡Œé‡‡æ · ç”Ÿæˆå¤šä¸ª<st>ï¼ˆrationaleï¼‰<et>ï¼Œ
  - å¼•å…¥â€œæ··åˆå¤´â€ï¼ˆmixing headï¼‰ï¼Œä¸€ä¸ªæµ…å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œç”¨äºç”Ÿæˆæƒé‡ï¼Œå†³å®šåœ¨ç»™å®šrationaleåï¼Œæ¨¡å‹åº”è¯¥åœ¨å¤šå¤§ç¨‹åº¦ä¸Šç»“åˆrationaleç”Ÿæˆçš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ¦‚ç‡å’ŒåŸºç¡€è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡ã€‚è¿™ä¸ªæ˜¯åœ¨hiddenä¸Šåšçš„ï¼Œä¸æ˜¯æ–‡æœ¬åˆ†ç±»å™¨
  - RL We thus define the reward rj for each rationale Tj as the difference between p talk j:j+ntrue and the average across rationales for that token
- V-StaR
  - ç°æœ‰çš„è‡ªæˆ‘æ”¹è¿›æ–¹æ³•ï¼ˆå¦‚STaRï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œå¿½ç•¥äº†å¤§é‡ç”Ÿæˆçš„é”™è¯¯è§£å†³æ–¹æ¡ˆã€‚è¿™äº›é”™è¯¯è§£å†³æ–¹æ¡ˆå¯èƒ½åŒ…å«æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ å¹¶æ”¹è¿›å…¶æ¨ç†è¿‡ç¨‹ã€‚
  - é‚£ä¹ˆæ€ä¹ˆåˆ©ç”¨é”™è¯¯æ–¹æ¡ˆå‘¢ï¼Œå¯ä»¥ä½¿ç”¨DPOï¼ˆDirect Preference Optimizationï¼‰æ–¹æ³•ç›´æ¥ç”¨äºè®­ç»ƒéªŒè¯å™¨
  - combines the approach of STaR  with a DPO trained verifer. At inference time, the STaR model produces several candidate reasoning chains (plans) which are ranked by the DPO verifer likelihood
- PRM-800k https://arxiv.org/abs/2305.20050
  - ç¬¬ä¸€æ¬¡æå‡º ç»“æœç›‘ç£ï¼ˆoutcome supervisionï¼‰ORM å’Œè¿‡ç¨‹ç›‘ç£ï¼ˆprocess supervisionï¼‰PRMæ¦‚å¿µ
  - ç›´æ¥äººå·¥æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«800,000ä¸ªstep-wiseçš„äººç±»åé¦ˆæ ‡ç­¾çš„å®Œæ•´æ•°æ®é›†ï¼ˆPRM800Kï¼‰ï¼Œæ²¡æœ‰å¼€æº
- MCTS æµç¨‹ç†è§£ï¼š https://mp.weixin.qq.com/s/BrLxo_p07zX6AqmGtUoVmw

## Data-Mining

- D-CPT Law: Domain-specific Continual Pre-Training Scaling Law
- Instruction-Mining
- [domain upsample](https://arxiv.org/pdf/2406.03476) 
- LLaMA3.1 tech report æ•°æ®æ¸…ç†ç­–ç•¥ï¼š
	- Post-training
		- preference dataï¼šå¤šä¸ªæ¨¡å‹ç”Ÿæˆï¼Œè®©annotatorså»æ ‡æ³¨æˆ–è€…editï¼Œç”¨edited > chosen > rejectedæ’åº, åªé‡‡æ · chosenæ˜æ˜¾å¥½äºrejectedçš„æ•°æ®ï¼Œé˜²æ­¢æ··æ·†, å¹¶ä¸”åœ¨æ¯ä¸€è½®éƒ½ä¼šåŠ å¤§éš¾åº¦
		- SFT data: ç”¨reward modelé€‰æ‹© æœ€æ–°æ¨¡å‹çš„å¯¹è¯å›å¤ ï¼Œå¹¶åœ¨åæœŸåŠ system promptå¼•å¯¼é£æ ¼è¯­æ°”
		- data cleanï¼š identify overused phrases (such as â€œIâ€™m sorryâ€ or â€œI apologizeâ€)ï¼Œ excessive use of emojis or exclamation points
		- data pruning:  ï¼ˆtopicï¼‰llama8b ä½œä¸ºtopic classifierï¼›ï¼ˆqualityï¼‰ llama3 2/3ä¸ªçº§åˆ«çš„è´¨é‡æ‰“åˆ†ï¼Œä»¥åŠreward model å‰1/4çš„æ‰“åˆ† ä¸¤è€… **æˆ–å…³ç³»**ï¼› ï¼ˆdifficultyï¼‰Instag æ„å›¾æ•°é‡å’Œ Llama 3ä¸ªçº§åˆ«çš„æ‰“åˆ†ï¼›(semantic deduplication) RoBERTa cluster
		- average models: branch-train-mix
		- æŒ‰ç…§Llama 2çš„åšæ³•ï¼Œæˆ‘ä»¬åº”ç”¨ä¸Šè¿°æ–¹æ³•è¿›è¡Œå…­è½®è¿­ä»£ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†æ–°çš„åå¥½æ³¨é‡Šå’ŒSFTæ•°æ®ï¼Œä»æœ€æ–°æ¨¡å‹ä¸­é‡‡æ ·åˆæˆæ•°æ®ã€‚
	- Pre-training
		- data mixï¼š contains roughly 50% of tokens corresponding to general knowledge, 25% of mathematical and reasoning tokens, 17% code tokens, and 8% multilingual tokens.
		- data annealing: åœ¨å¤§æ¨¡å‹è®­ç»ƒçš„æœ€åé˜¶æ®µï¼Œç”¨é«˜è´¨é‡çš„æ•°æ®å­¦ä¹ èƒ½æé«˜æ€§èƒ½ã€‚äºæ˜¯åœ¨æœ€å40Bæ•°æ®ä¸Šï¼Œä½œè€…é€æ¸å°†å­¦ä¹ ç‡è¡°å‡åˆ°0ã€‚å¹¶ä¸”ä½œè€…å‘ç°ï¼Œæ•°æ®é€€ç«æ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥ç­›æ•°æ®
		- long-context pretraining: ç”¨6ä¸ªstage é€æ­¥å°†é•¿åº¦ä»8kæ‰©å±•åˆ°128kï¼Œå¹¶ä¸”åŠ attention maské¿å…ä¸åŒæ•°æ®ä¸²å‘³ï¼ˆå¯¹é•¿æ–‡å½±å“å¾ˆå¤§ï¼‰
		- æ•°æ®çš„å®‰å…¨æ€§å’Œè´¨é‡ï¼Œweb data curation
			1. è¿‡æ»¤å™¨ç§»é™¤å¯èƒ½å«æœ‰ä¸å®‰å…¨å†…å®¹æˆ–å¤§é‡ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰çš„ç½‘ç«™æ•°æ®ï¼Œä»¥åŠæ ¹æ®å¤šç§Metaå®‰å…¨æ ‡å‡†è¢«è¯„ä¸ºæœ‰å®³çš„åŸŸåå’Œå·²çŸ¥å«æœ‰æˆäººå†…å®¹çš„åŸŸåã€‚
			2. ä½¿ç”¨è‡ªå®šä¹‰è§£æå™¨å¤„ç†éæˆªæ–­çš„ç½‘é¡µæ–‡æ¡£ åœ¨URLã€æ–‡æ¡£å’Œè¡Œçº§åˆ«è¿›è¡Œå¤šè½®å»é‡ï¼šä¿ç•™æ¯ä¸ªURLå¯¹åº”çš„æœ€æ–°ç‰ˆæœ¬é¡µé¢ã€‚ä½¿ç”¨å…¨å±€MinHashå»é‡ï¼Œç§»é™¤è¿‘ä¼¼é‡å¤çš„æ–‡æ¡£ã€‚è¿›è¡Œç±»ä¼¼ccNetçš„lineçº§åˆ«å»é‡ï¼Œç§»é™¤åœ¨æ¯3000ä¸‡æ–‡æ¡£æ¡¶ä¸­å‡ºç°è¶…è¿‡6æ¬¡çš„line
			3. å¼€å‘å¯å‘å¼è§„åˆ™ç§»é™¤é¢å¤–çš„ä½è´¨é‡æ–‡æ¡£ã€å¼‚å¸¸å€¼å’Œé‡å¤è¿‡å¤šçš„æ–‡æ¡£ã€‚ä½¿ç”¨é‡å¤n-gramè¦†ç›–ç‡å»é™¤ç”±æ—¥å¿—æˆ–é”™è¯¯æ¶ˆæ¯ç»„æˆçš„é‡å¤å†…å®¹è¡Œï¼Œä½¿ç”¨â€œdirty wordâ€è®¡æ•°è¿‡æ»¤æœªè¢«åŸŸåé˜»æ­¢åˆ—è¡¨è¦†ç›–çš„æˆäººç½‘ç«™ï¼Œä»¥åŠä½¿ç”¨ä»¤ç‰Œåˆ†å¸ƒçš„KL divergenceè¿‡æ»¤å«æœ‰å¼‚å¸¸æ•°é‡çš„å¼‚å¸¸ä»¤ç‰Œçš„æ–‡æ¡£ã€‚
			4. **åŸºäºæ¨¡å‹çš„è´¨é‡è¿‡æ»¤**ï¼šå®éªŒæ€§åœ°åº”ç”¨å„ç§åŸºäºæ¨¡å‹çš„è´¨é‡åˆ†ç±»å™¨æ¥ç­›é€‰é«˜è´¨é‡çš„æ ‡è®°ã€‚åŒ…æ‹¬ä½¿ç”¨fasttextå¿«é€Ÿåˆ†ç±»å™¨è¯†åˆ«å¯èƒ½è¢«ç»´åŸºç™¾ç§‘å¼•ç”¨çš„æ–‡æœ¬ï¼ŒåŸºäºRobertaçš„åˆ†ç±»å™¨ï¼Œå®ƒä»¬åœ¨Llama 2é¢„æµ‹ä¸Šè¿›è¡Œè®­ç»ƒã€‚
			5. **ä»£ç å’Œæ¨ç†æ•°æ®**ï¼šç±»ä¼¼äºDeepSeek-AIç­‰ï¼Œæ„å»ºç‰¹å®šé¢†åŸŸçš„ç®¡é“æå–ä»£ç å’Œä¸æ•°å­¦ç›¸å…³çš„ç½‘é¡µã€‚ä»£ç å’Œæ¨ç†åˆ†ç±»å™¨éƒ½æ˜¯åŸºäºLlama 2æ ‡æ³¨çš„ç½‘é¡µæ•°æ®è®­ç»ƒçš„DistilledRobertaæ¨¡å‹ã€‚
			6. **å¤šè¯­è¨€æ•°æ®**ï¼šä½¿ç”¨åŸºäºfasttextçš„è¯­è¨€è¯†åˆ«æ¨¡å‹å°†æ–‡æ¡£åˆ†ç±»ä¸º176ç§è¯­è¨€ã€‚

## Init

- Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer https://thegregyang.com/
	- ä¿è¯å‚æ•°åˆå§‹åŒ–+ä¼˜åŒ–å™¨ ä½¿å¾—å…¶å’Œwidth æ— å…³, hiddenæ˜¯ä¸¤ä¸ªéƒ½æ— ç©·ï¼Œinputå’Œoutputæ˜¯åªæœ‰ä¸€ä¸ªæ— ç©·, ä¸¤ä¸ªéƒ½æ— ç©·ç”¨çš„ä¸­å¿ƒæé™ï¼Œä¸€ä¸ªæ— ç©·ç”¨çš„å¤§æ•°å®šå¾‹
   	- ÂµP èƒ½å¤Ÿç¡®ä¿åœ¨æ¨¡å‹å¤§å°å˜åŒ–æ—¶ï¼Œè®¸å¤šæœ€ä¼˜è¶…å‚æ•°ä¿æŒç¨³å®šã€‚è¿™ä½¿å¾—å¯ä»¥ä»è¾ƒå°æ¨¡å‹é—´æ¥è°ƒä¼˜è¶…å‚æ•°ï¼Œç„¶åé›¶æ ·æœ¬ï¼ˆzero-shotï¼‰è¿ç§»åˆ°å…¨å°ºå¯¸æ¨¡å‹ä¸Š
  

## LoRA

- æ¶æ„
	- VeRA: freezes random weight tied adapters and learns vector scalings of the internal adapter activations.
	- LoRA-XS: initializes the A and B matrices using the SVD of the pretrained weights and trains a low-rank update of the form BRA where R is a trainable r Ã— r matrix and B, A are fixed.
	- NOLA: parametrizes the adapter matrices to be linear combinations of frozen random matrices and optimizes the linear coefficients of the mixtures.
	- VB-LORA: shares adapter parameters using a global vector bank.
	- MoRA: learns high-rank updates while still preserving parameter efficiency by applying hand-designed compress and decompress operations before and after a trainable adapter matrix.
	- DoRA: decomposes the pretrained weight into magnitude and direction components to allow for better training dynamics
	- GaLoRe: ä½¿ç”¨SVDå°†å…¨å‚æ•°è®­ç»ƒçš„æ¢¯åº¦æŠ•å½±åˆ°ä½ç§©ç©ºé—´
	- IA3ï¼ˆImplicit Activation Scalingï¼‰: é€šè¿‡ä¿®æ”¹æ¿€æ´»å‘é‡çš„ç¼©æ”¾æ¥é€‚åº”æ¨¡å‹ï¼Œè€Œä¸æ˜¯è°ƒæ•´æƒé‡ã€‚
- è®­ç»ƒæ”¹è¿›
	- LoRA-FA: freezes the A matrix which leads to small performance loss while reducing memory consumption
	  by up to 1.4Ã—.  
	- https://arxiv.org/pdf/2406.08447v1 [initA] > [initB] é€šè¿‡å¯¹ç¥ç»ç½‘ç»œå®½åº¦æé™çš„ç†è®ºåˆ†æï¼ˆuPï¼‰
	- LoRA+: åŒæ ·ç ”ç©¶æ— é™å®½åº¦ä¸‹çš„åˆå§‹åŒ–ï¼Œç»“è®ºæ˜¯ç»™ABä¸åŒçš„å­¦ä¹ ç‡
	- Pissaï¼šå¯¹W0åšSVDæ¥åˆå§‹åŒ–A,B
	- LoRA-GAï¼šå°½é‡å¯¹é½ç¬¬ä¸€æ­¥æ›´æ–°åçš„W1ï¼Œå¯¹åˆå§‹æ¢¯åº¦G0=âˆ‡W0LåšSVDï¼Œå–Uçš„å‰råˆ—åˆå§‹åŒ–Aï¼Œå–Vçš„ç¬¬r+1âˆ¼2rè¡Œåˆå§‹åŒ–B
	- **LoRA-Pro**: å¯¹é½å…¨é‡å¾®è°ƒå’ŒLoRAçš„æ¯ä¸€ä¸ªWt,
- æ•ˆæœ
	- https://arxiv.org/pdf/2405.09673 LoRAåœ¨ç›®æ ‡é¢†åŸŸçš„æ€§èƒ½é€šå¸¸ä½äºå…¨å‚æ•°å¾®è°ƒï¼Œä½†åœ¨ä¿æŒæºé¢†åŸŸæ€§èƒ½æ–¹é¢è¡¨ç°æ›´å¥½ï¼›LoRAæä¾›äº†æ¯”ä¼ ç»Ÿæ­£åˆ™åŒ–æŠ€æœ¯(finetuned, weight-decay)æ›´å¼ºçš„æ­£åˆ™åŒ–æ•ˆæœï¼Œå¹¶æœ‰åŠ©äºä¿æŒç”Ÿæˆå¤šæ ·æ€§
  - QLoRA: matched full finetuning MMLU (Hendrycks et al., 2020) performance, optimized LoRA configurations perform as well as full finetuning, and that performance is governed by choice of target modules but not rank.
  - DoRA: shows that LoRA is sensitive to ranks. It is likely that some of these discrepancies
    are due to differences in finetuning datasets and evaluations.  
- é¢„è®­ç»ƒ
	- SwitchLoRAï¼šæ„é€ min(m,n)ä¸ªå€™é€‰çš„è¡Œå‘é‡å’Œåˆ—å‘é‡ï¼Œç„¶åæ¯æ­¥éšæœºå–ä¸€ä¸ªæ’åˆ°Aå’ŒBä¸Šè®­ç»ƒï¼Œä¿è¯è®­ç»ƒæ»¡ç§©
## Decoding


- Learning to Decode Collaboratively with Multiple Language Models [[paper]](https://arxiv.org/abs/2403.03870)


- Survey: Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding [[paper]](https://arxiv.org/abs/2401.07851)
- Fast Inference from Transformers via Speculative Decoding [[paper]](https://arxiv.org/pdf/2211.17192)[[repo]](https://github.com/feifeibear/LLMSpeculativeSampling) 
- Accelerating Large Language Model Decoding with Speculative Sampling [[paper]](https://arxiv.org/pdf/2302.01318)
- [ASPLOS'24] SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification [[paper]](https://arxiv.org/abs/2305.09781)
- [ICML24] EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty [[paper]](https://arxiv.org/pdf/2401.15077) [[blog]](https://sites.google.com/view/eagle-llm)
- EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees [[paper]](https://arxiv.org/pdf/2406.16858)
- [ICLR24] DistillSpec: Improving Speculative Decoding via Knowledge Distillation [[paper]](https://arxiv.org/abs/2310.08461) ç”¨target modelä½œä¸ºteacherå¯¹draft modelè’¸é¦
- [NAACL24] REST: Retrieval-Based Speculative Decoding [[paper]](https://arxiv.org/pdf/2311.08252)
- Graph-Structured Speculative Decoding [[paper]](https://arxiv.org/pdf/2407.16207)

## Long-Context


- ç›¸å¯¹åç½®
	- ALIBI [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409), ICLR22
	  > åœ¨Softmaxä¹‹å‰å‡å»ä¸€ä¸ªéè´ŸçŸ©é˜µï¼Œå°†Attentionçš„è®¡ç®—ä»$q_{m} k_{n}$æ”¹ä¸º $q_{m} k_{n}-\lambda|m-n|$ï¼Œå…¶ä¸­Î»>0æ˜¯è¶…å‚æ•°ï¼Œæ¯ä¸ªheadè®¾ç½®ä¸åŒçš„å€¼
- base ç¼©æ”¾
	- Position Interpolation (PI) [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595)
	  > ä½ç½®ç¼–ç baseï¼ˆé»˜è®¤ä¸º10000ï¼‰ä¹˜ä¸Šå› å­$L_{train}/L_{test}$
	- Dynamic-NTK  [Reddit](www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
	  > PIçš„ç¼©æ”¾æ˜¯å¹³ç­‰å¯¹å¾…åœ°å¯¹å¾…æ‰€æœ‰ç»´åº¦ï¼Œå³é«˜é¢‘æ—‹è½¬è§’åº¦ç¼©å°çš„å€æ•°å’Œä½é¢‘æ—‹è½¬è§’åº¦ç¼©å°çš„å€æ•°æ˜¯ä¸€æ ·çš„ã€‚ NTK-Aware Scaled RoPE å¯ä»¥ç†è§£ä¸ºå¯¹ä½é¢‘å†…æ’ï¼Œé«˜é¢‘å¤–æ’
	- [YaRN: Efficient Context Window Extension of Large Language Models](https://openreview.net/forum?id=wHBfxhZu1u), ICLR 2024
	  > 1. å¦‚æœç»´åº¦iå¯¹åº”çš„æ³¢é•¿$$\lambda_i$$è¿œå°äºæ–‡æœ¬é•¿åº¦ï¼Œä¸è¿›è¡Œå†…æ’åªå¤–æ¨ 2. å¦‚æœç»´åº¦iå¯¹åº”çš„æ³¢é•¿$$\lambda_i$$å¤§äºæ–‡æœ¬é•¿åº¦ï¼Œè¿›è¡Œå†…æ’ 3. å¯¹äºä¸­é—´éƒ¨åˆ†ï¼Œé‡‡ç”¨NTK-Aware Scaled RoPEçš„æ€è·¯
	- [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://openreview.net/forum?id=ONOtpXLqqw), ICML 2024
	  > RoPE çš„ä¸åŒç»´åº¦å­˜åœ¨ä¸å‡è¡¡æ€§, ç”¨è¿›åŒ–ç®—æ³•æœç´¢éå‡åŒ€ä½ç½®æ’å€¼
- åˆ†å— / chunk
	- MRC / Long Text Match é‡Œè¾¹æœ‰å¾ˆå¤š
	- [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
	  >  ä»¥ä¸€ä¸ªå›ºå®šé•¿åº¦åˆ’åˆ†å¤šä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£å†…éƒ¨åš Attention, æŠŠåˆ†çª—å£æ³¨æ„åŠ›çš„ Attention mask å‘ä¸‹ç§»åŠ¨åŠä¸ªçª—å£çš„é•¿åº¦ï¼Œè®©ä¸Šé¢æåˆ°çš„ä¸åŒçš„çª—å£ä¹‹é—´è¿›è¡Œäº¤äº’; å¹¶ä¸”LORAå¢åŠ å¯¹LayerNorm å’Œ Embedding çš„å¾®è°ƒ
	- [LongHeads: Multi-Head Attention is Secretly a Long Context Processor](https://arxiv.org/abs/2402.10685)
	  > è®¤ä¸ºä¸åŒçš„æ³¨æ„åŠ›å¤´æ‰€å…³æ³¨çš„æ˜¯ context ä¸­çš„ä¸åŒéƒ¨åˆ†ï¼Œå°†é•¿ context åˆ†è§£æˆå¤šä¸ªå—ï¼Œè®©æ¯ä¸ªæ³¨æ„åŠ›å¤´åˆ†åˆ«å…³æ³¨é‡è¦çš„å—ï¼Œå¹¶ä¿è¯åˆ†é…ç»™æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„é‚£äº›å—åŒ…å«çš„ tokens æ•°å°‘äºé¢„è®­ç»ƒçš„çª—å£å¤§å°
	- [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](https://openreview.net/forum?id=ulaUJFd96G), ICLR24
	  >  å°†é•¿æ–‡æœ¬è¾“å…¥åˆ†å‰²æˆå¤šä¸ªå°å—ï¼ˆchunksï¼‰, å±‚æ¬¡åŒ–åˆå¹¶ç­–ç•¥ï¼Œé€šè¿‡åœ¨ä¸åŒçš„transformerå±‚çº§é€æ­¥åˆå¹¶ç›¸é‚»çš„å—ï¼Œä½¿å¾—ä¿¡æ¯å¯ä»¥åœ¨å—ä¹‹é—´ä¼ é€’ã€‚
	- [Training-Free Long-Context Scaling of Large Language Models](https://arxiv.org/abs/2402.17463), ICML 2024
	  >  æŒ‰å—è¿›è¡Œæ—‹è½¬ç¼–ç ï¼ˆRoPEï¼‰ï¼Œæå‡ºä¸€ç§å—å†…ï¼Œå—é—´ä»¥åŠç›¸é‚»å—Attentionçš„DCAç­–ç•¥
	- [LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning](https://openreview.net/forum?id=nkOMLBIiI7), ICML2024
	  > åˆ©ç”¨ â€œgrouped attentionâ€  è·ç¦»è¿‘çš„ç”¨åŸæ¥çš„attentionï¼Œè¿œçš„è¯ç”¨grouped attention


## KV-Cache

- Survey: https://github.com/October2001/Awesome-KV-Cache-Compression
- KV Cache Quantization
	- Coupled Quantization (Zhang et al., 2024b). and KIVI (Zirui Liu et al., 2023), have demonstrated that KV cache can be quantized to 1-bit or 2-bit precision while preserving performance.
	- [IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact](https://arxiv.org/abs/2403.01241)
	  > Pivot tokens ä½œä¸ºé‡è¦çš„å‰ç¼€ï¼Œä¸åº”è¯¥è¿›è¡Œé‡åŒ–  
- KV Cache Low-Rank
	- insights: è®¤ä¸ºKV cacheæ˜¯ä½ç§©çš„
	- [Effectively Compress KV Heads for LLM](https://arxiv.org/abs/2406.07056)
	  > (1) only 25% of the highest singular values need to be retained to get most of the energy.   
	  (2) RoPE generally reduces the rank of key cache  
	  éœ€è¦æ•°æ®å¹¶ä¸”åœ¨æ¿€æ´»å€¼ä¸Šè¿›è¡ŒSVDçš„æ–¹æ³•ç§°ä¸ºSVD-aï¼Œç›´æ¥åœ¨æƒé‡çŸ©é˜µä¸ŠåšSVDç§°ä¸ºSVD-w  
	- LESS: [Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2402.09398)
	  > ç”¨ä½ç§©çŸ©é˜µæ¥æ‹Ÿåˆ kv cache token dropping å¸¦æ¥çš„è¯¯å·®ï¼Œattention map çš„è¯¯å·®çŸ©é˜µå¾€å¾€æ˜¯ä½ç§©çš„ï¼Œå› æ­¤å¯ä»¥ç±»ä¼¼ Linear Attention çš„åšæ³•ï¼ŒæŠŠ softmax ä¸­è¡¨ç¤ºç›¸ä¼¼åº¦çš„æŒ‡æ•°éƒ¨åˆ†ï¼Œæ›´æ¢ä¸ºåˆ†åˆ«å¯¹ qk è¿›è¡Œå˜æ¢ç„¶ååšå†…ç§¯  
	- [GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527)
	  > ç”¨ä½ç§©çŸ©é˜µæ‹Ÿåˆkv cacheé‡åŒ–å¸¦æ¥çš„è¯¯å·®ï¼Œå› ä¸ºé‡åŒ–å¯¼è‡´çš„è¯¯å·®çŸ©é˜µç§©æ¯”è¾ƒä½ï¼Œå¯ä»¥ç”¨ä¸¤ä¸ªçŸ©é˜µæ‹Ÿåˆ  
- KV Cache Eviction
	- insights: attentionæœ¬èº«å…·æœ‰çš„ç¨€ç–æ€§, 50%çš„ KV cacheè´¡çŒ®äº†0.9ä»¥ä¸Šçš„ Attention Scores
	- Scissorhands (Liu et al., 2023b)
	  keeps a fixed KV size budget and replies on the Persistence of Importance hypothesis to evict key  
	  and value states for non-important tokens  
	- [H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf)ï¼ŒNeurIPS 2023
	  > 20%ç¼“å­˜æ¥è¿‘äºå…¨é‡KV cacheçš„æ•ˆæœ;   
	  utilizes aggregated attention scores to determine so called â€œheavy hittersâ€/ important tokens  
	- StreamingLLM [Efficient Streaming Language Models with Attention Sinks](http://arxiv.org/abs/2309.17453), ICLR 2024
	  > attention sink + the recent window tokens is pivotal to maintain LLMâ€™s performance  
	- FastGen [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://arxiv.org/abs/2310.01801)
	> attention sinks also occurs in the middle of the sentences
	è®¤ä¸ºæ³¨æ„åŠ›å¤´åœ¨ä¸åŒä½ç½®ä¸‹çš„ attention map ç»“æ„æ˜¯ç›¸å¯¹ç¨³å®šçš„ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡è¾“å…¥çš„ prompt æ¥ç¡®å®šæ³¨æ„åŠ›å¤´çš„å…¨å±€æ¨¡å¼
	- VATP [Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters](https://arxiv.org/abs/2406.12335)
	  > attention sink tokens å¯¹åº”çš„å€¼å‘é‡çš„ $l_1$ normæ¯”å…¶ä»–å°å¾ˆå¤š, åŸºäºattention score å’Œå€¼å‘é‡çš„L1èŒƒæ•°çš„ä¹˜ç§¯æ¥æŒ‘é€‰KV cache  
	- [Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference](https://arxiv.org/pdf/2403.09054) 
	  > H2O æ”¹è¿› ä½¿ç”¨Gumbelåˆ†å¸ƒå¼•å…¥å™ªå£°ä»¥è°ƒæ•´æœªå½’ä¸€åŒ–logitsï¼Œä»è€Œè§£å†³ç”±äºä¸¢å¼ƒtokenè€Œå¯¼è‡´çš„uneven score distribution é—®é¢˜ã€‚  
- KV Cache Merging ï¼ˆToken Merging, Token Pooling, Token Pruningï¼‰
	- insightsï¼š
		- directly eviction may accidentally and permanently remove important tokens;
		- key states exhibit high similarity at the token level within a single sequence
	- token merging is well-established in computer vision (CV)
	  (Zeng et al., 2022) (Bolya et al., 2023) (Kim et al., 2023) (Zhang et al., 2024a),  
	- [CaM: Cache Merging for Memory-efficient LLMs Inference](https://openreview.net/pdf?id=LCTmppB165), ICML 2024. 
	  > ä¸æ˜¯ç›´æ¥å°†å…¶éœ€è¦é€å‡ºçš„tokenä¸¢å¼ƒï¼Œè€Œæ˜¯é€šè¿‡mergeæ¥åˆ©ç”¨é€å‡ºçš„å…ƒç´   
	  paperé‡Œè¾¹ç†è®ºè¯æ˜å¥½å¤„åœ¨äºå¯¹attentionçš„è¾“å‡ºæ‰°åŠ¨æ›´å°ã€‚  
	- [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636), ICML 2024
	  > å°†æ¯æ¬¡æ–°è¿›å…¥çš„KV mergeï¼Œ å¯¹äºæ¯ä¸ªæ–°æ¥çš„kvï¼Œå†³å®šæ˜¯mergeè¿˜æ˜¯append  
	- [LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference](https://openreview.net/pdf/d775ca7f5d0bfad0e56d5e710a3953555ccaabda.pdf)
	  > åº”ç”¨åœ¨MLLMä¸Š (è¾“å…¥æ˜¯interleavedçš„å›¾æ–‡å¯¹)ï¼Œæ–‡æœ¬ä¸æ“ä½œï¼Œå¯¹å›¾åƒtoken merge å› ä¸ºåœ¨å¤šæ¨¡æ€ä¸­æ–‡æœ¬çš„ç›¸å¯¹å›¾ç‰‡å…·æœ‰æ›´é«˜çš„attention scoreï¼Œæå‡º4ç§mergeç­–ç•¥ï¼šMaxï¼ŒMeanï¼ŒPivotal ï¼Œweighted  
	- [KVMerger: Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks](https://arxiv.org/abs/2407.08454)
	  > è®¤ä¸ºä¹‹å‰çš„å·¥ä½œåœ¨identity merge setä¸Šå­˜åœ¨ç¼ºé™·ï¼Œå‘ç°KV cacheçš„å‹ç¼©æ¯”ç‡åœ¨ä¸åŒæ ·æœ¬ä¸Šé«˜åº¦ä¸€è‡´(æ¨¡å‹å›ºæœ‰ç‰¹æ€§)ï¼Œå› æ­¤å¯ä»¥ç›´æ¥ç”¨layer-wiseçš„cos-simé™æ€è®¡ç®—å‹ç¼©æ¯”ç‡ï¼›åœ¨LLMsçš„å‰ä¸¤å±‚å’Œæœ€åä¸€å±‚æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒæ›´åŠ å‡åŒ€ï¼Œæ„å‘³ç€å¤§å¤šæ•°é”®çŠ¶æ€éƒ½å¾ˆé‡è¦ï¼Œåº”è¯¥ä¸mergingä»¥é¿å…å¼•å…¥æ˜¾è‘—çš„å™ªå£°ï¼› Gaussian kernel weighted merging algorithm  
- KV Cache CrossAttn
	- CLA [Reducing Transformer Key-Value Cache Size with Cross-Layer Attention](https://arxiv.org/abs/2405.12981)
	  > ç›¸é‚»layer å…±äº«KV cache  

## Model-Merging

- survey: https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications

## Image-Tokenizer

## Interpretable

- [ACL25] https://arxiv.org/pdf/2410.23743  æ­£æ–‡8é¡µä½†æ˜¯é™„å½•æœ‰123é¡µ
	- **Nuclear Norm** (the â„“1 norm of singular values) to represent the characteristics for the gradient of each layer $$s_{X,i} = \sum |\sigma_j|, X \in \{QKVO\}, i \in [0, N-1]$$ , N is layernum, QKVO is attention matrix
	- **mean absolute differences (MAD)** is defined as $$\mathrm{MAD}_{s_X}=\frac{1}{N-1} \sum_{i=1}^{N-1}\left|s_{X, i+1}-s_{X, i}\right|$$
	- None CoT / Simplified CoT / Detailed CoT (Slow vs. Fast Thinking)
		 - The large scale of MAD indicates that the response distributions that LLMs are going to learn have large discrepancies with what it has learned from the pretraining phase, which might harm the performances of the original pre-trained models
		- A consistent decrease in MAD is observed in all layers when LLMs are trained to produce more detailed reasoning paths (slow thinking)
		- LLMs can to some extent identify that the responses to be learned have potential conflicts
		  with their internal knowledge, thus requiring more energy to adapt to the new nonsense responses.


## Squence Parallel
- data/tensor/zero/expert/pipeline parallelism.
- Sequence Parallelism of Megatron-LM:  this form of Sequence Parallelism cannot be used independently without tensor parallelism
- [Deepspeed-Ulysses](https://arxiv.org/abs/2309.14509) P2P communication
- [Ring-Attention](https://arxiv.org/pdf/2310.01889) a distributed version of FlashAttention, All2All communication;
	- Context Parallel https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/context_parallel.html
- [USP-Attention](https://arxiv.org/pdf/2405.07719) 
